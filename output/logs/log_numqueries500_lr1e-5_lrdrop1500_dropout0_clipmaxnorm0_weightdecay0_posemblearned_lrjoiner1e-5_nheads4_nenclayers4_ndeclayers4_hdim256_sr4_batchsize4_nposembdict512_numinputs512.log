Logging output to output/logs/log_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512.log
Traceback (most recent call last):
  File "src/main.py", line 10, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'
Logging output to output/logs/log_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512.log
Namespace(activation='leaky_relu', aux_loss=False, batch_size=4, clip_max_norm=0.0, cuda=False, data_root='/local-scratch/localhome/aabdujyo/activitygraph_transformer/data/thumos', dataset='thumos', dec_layers=4, device='cpu', dim_feedforward=1024, dim_latent=128, dist_url='env://', dropout=0.0, enc_layers=4, eos_coef=0.1, epochs=10, eval=False, features='i3d_feats', hidden_dim=256, local_rank=None, lr=1e-05, lr_drop=1500, lr_joiner=1e-05, model='agt', nheads=4, norm_type='bn', num_classes=20, num_inputs=512, num_pos_embed_dict=512, num_queries=500, num_workers=0, output_dir='output/checkpoints_/checkpoints_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512', position_embedding='learned', pre_norm=False, resume='', sample_rate=4, save_checkpoint_every=30, seed=42, segment_loss_coef=5.0, set_cost_class=1, set_cost_segment=5.0, set_cost_siou=3.0, siou_loss_coef=3.0, start_epoch=0, step_size=1, weight_decay=0.0, world_size=1)
AGT(
  (transformer): Graph_Transformer(
    (encoder): GraphTransformerEncoder(
      (layers): ModuleList(
        (0): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (1): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (2): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (3): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (decoder): GraphTransformerDecoder(
      (layers): ModuleList(
        (0): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
        (1): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
        (2): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
        (3): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
      )
      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
  )
  (joiner): Joiner(
    (0): PositionEmbeddingLearned(
      (row_embed): Embedding(512, 256)
    )
  )
  (query_embed): Embedding(500, 256)
  (class_embed): Linear(in_features=256, out_features=21, bias=True)
  (segments_embed): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
      (2): Linear(in_features=256, out_features=2, bias=True)
    )
  )
  (input_proj): Conv1d(2048, 256, kernel_size=(1,), stride=(1,))
)
number of params: 10150935
Traceback (most recent call last):
  File "src/main.py", line 204, in <module>
    main(args)
  File "src/main.py", line 138, in main
    sampler_train = torch.utils.data.RandomSampler(dataset_train)
  File "/localhome/aabdujyo/anaconda3/envs/agt_env/lib/python3.8/site-packages/torch/utils/data/sampler.py", line 93, in __init__
    raise ValueError("num_samples should be a positive integer "
ValueError: num_samples should be a positive integer value, but got num_samples=0
Logging output to output/logs/log_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512.log
Namespace(activation='leaky_relu', aux_loss=False, batch_size=4, clip_max_norm=0.0, cuda=False, data_root='/home/aabdujyo/scratch/activitygraph_transformer/data/thumos', dataset='thumos', dec_layers=4, device='cpu', dim_feedforward=1024, dim_latent=128, dist_url='env://', dropout=0.0, enc_layers=4, eos_coef=0.1, epochs=10, eval=False, features='i3d_feats', hidden_dim=256, local_rank=None, lr=1e-05, lr_drop=1500, lr_joiner=1e-05, model='agt', nheads=4, norm_type='bn', num_classes=20, num_inputs=512, num_pos_embed_dict=512, num_queries=500, num_workers=0, output_dir='output/checkpoints_/checkpoints_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512', position_embedding='learned', pre_norm=False, resume='', sample_rate=4, save_checkpoint_every=30, seed=42, segment_loss_coef=5.0, set_cost_class=1, set_cost_segment=5.0, set_cost_siou=3.0, siou_loss_coef=3.0, start_epoch=0, step_size=1, weight_decay=0.0, world_size=1)
AGT(
  (transformer): Graph_Transformer(
    (encoder): GraphTransformerEncoder(
      (layers): ModuleList(
        (0): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (1): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (2): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (3): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (decoder): GraphTransformerDecoder(
      (layers): ModuleList(
        (0): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
        (1): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
        (2): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
        (3): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
      )
      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
  )
  (joiner): Joiner(
    (0): PositionEmbeddingLearned(
      (row_embed): Embedding(512, 256)
    )
  )
  (query_embed): Embedding(500, 256)
  (class_embed): Linear(in_features=256, out_features=21, bias=True)
  (segments_embed): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
      (2): Linear(in_features=256, out_features=2, bias=True)
    )
  )
  (input_proj): Conv1d(2048, 256, kernel_size=(1,), stride=(1,))
)
number of params: 10150935
Traceback (most recent call last):
  File "src/main.py", line 204, in <module>
    main(args)
  File "src/main.py", line 138, in main
    sampler_train = torch.utils.data.RandomSampler(dataset_train)
  File "/home/aabdujyo/softwares/anaconda3/envs/agt_env/lib/python3.8/site-packages/torch/utils/data/sampler.py", line 93, in __init__
    raise ValueError("num_samples should be a positive integer "
ValueError: num_samples should be a positive integer value, but got num_samples=0
Logging output to output/logs/log_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512.log
Traceback (most recent call last):
  File "src/main.py", line 10, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'
Logging output to output/logs/log_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512.log
Namespace(activation='leaky_relu', aux_loss=False, batch_size=4, clip_max_norm=0.0, cuda=False, data_root='/home/aabdujyo/scratch/activitygraph_transformer/data/thumos', dataset='thumos', dec_layers=4, device='cpu', dim_feedforward=1024, dim_latent=128, dist_url='env://', dropout=0.0, enc_layers=4, eos_coef=0.1, epochs=10, eval=False, features='i3d_feats', hidden_dim=256, local_rank=None, lr=1e-05, lr_drop=1500, lr_joiner=1e-05, model='agt', nheads=4, norm_type='bn', num_classes=20, num_inputs=512, num_pos_embed_dict=512, num_queries=500, num_workers=0, output_dir='output/checkpoints_/checkpoints_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512', position_embedding='learned', pre_norm=False, resume='', sample_rate=4, save_checkpoint_every=30, seed=42, segment_loss_coef=5.0, set_cost_class=1, set_cost_segment=5.0, set_cost_siou=3.0, siou_loss_coef=3.0, start_epoch=0, step_size=1, weight_decay=0.0, world_size=1)
AGT(
  (transformer): Graph_Transformer(
    (encoder): GraphTransformerEncoder(
      (layers): ModuleList(
        (0): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (1): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (2): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (3): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (decoder): GraphTransformerDecoder(
      (layers): ModuleList(
        (0): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
        (1): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
        (2): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
        (3): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
      )
      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
  )
  (joiner): Joiner(
    (0): PositionEmbeddingLearned(
      (row_embed): Embedding(512, 256)
    )
  )
  (query_embed): Embedding(500, 256)
  (class_embed): Linear(in_features=256, out_features=21, bias=True)
  (segments_embed): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
      (2): Linear(in_features=256, out_features=2, bias=True)
    )
  )
  (input_proj): Conv1d(2048, 256, kernel_size=(1,), stride=(1,))
)
number of params: 10150935
Traceback (most recent call last):
  File "src/main.py", line 204, in <module>
    main(args)
  File "src/main.py", line 138, in main
    sampler_train = torch.utils.data.RandomSampler(dataset_train)
  File "/home/aabdujyo/softwares/anaconda3/envs/agt_env/lib/python3.8/site-packages/torch/utils/data/sampler.py", line 93, in __init__
    raise ValueError("num_samples should be a positive integer "
ValueError: num_samples should be a positive integer value, but got num_samples=0
Logging output to output/logs/log_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512.log
Namespace(activation='leaky_relu', aux_loss=False, batch_size=4, clip_max_norm=0.0, cuda=False, data_root='/home/aabdujyo/scratch/activitygraph_transformer/data/thumos', dataset='thumos', dec_layers=4, device='cpu', dim_feedforward=1024, dim_latent=128, dist_url='env://', dropout=0.0, enc_layers=4, eos_coef=0.1, epochs=10, eval=False, features='i3d_feats', hidden_dim=256, local_rank=None, lr=1e-05, lr_drop=1500, lr_joiner=1e-05, model='agt', nheads=4, norm_type='bn', num_classes=20, num_inputs=512, num_pos_embed_dict=512, num_queries=500, num_workers=0, output_dir='output/checkpoints_/checkpoints_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512', position_embedding='learned', pre_norm=False, resume='', sample_rate=4, save_checkpoint_every=30, seed=42, segment_loss_coef=5.0, set_cost_class=1, set_cost_segment=5.0, set_cost_siou=3.0, siou_loss_coef=3.0, start_epoch=0, step_size=1, weight_decay=0.0, world_size=1)
AGT(
  (transformer): Graph_Transformer(
    (encoder): GraphTransformerEncoder(
      (layers): ModuleList(
        (0): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (1): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (2): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (3): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (decoder): GraphTransformerDecoder(
      (layers): ModuleList(
        (0): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
        (1): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
        (2): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
        (3): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
      )
      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
  )
  (joiner): Joiner(
    (0): PositionEmbeddingLearned(
      (row_embed): Embedding(512, 256)
    )
  )
  (query_embed): Embedding(500, 256)
  (class_embed): Linear(in_features=256, out_features=21, bias=True)
  (segments_embed): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
      (2): Linear(in_features=256, out_features=2, bias=True)
    )
  )
  (input_proj): Conv1d(2048, 256, kernel_size=(1,), stride=(1,))
)
number of params: 10150935
/home/aabdujyo/scratch/activitygraph_transformer/data/thumos i3d_feats
/home/aabdujyo/scratch/activitygraph_transformer/data/thumos i3d_feats
Traceback (most recent call last):
  File "src/main.py", line 204, in <module>
    main(args)
  File "src/main.py", line 138, in main
    sampler_train = torch.utils.data.RandomSampler(dataset_train)
  File "/home/aabdujyo/softwares/anaconda3/envs/agt_env/lib/python3.8/site-packages/torch/utils/data/sampler.py", line 93, in __init__
    raise ValueError("num_samples should be a positive integer "
ValueError: num_samples should be a positive integer value, but got num_samples=0
Logging output to output/logs/log_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512.log
Namespace(activation='leaky_relu', aux_loss=False, batch_size=4, clip_max_norm=0.0, cuda=False, data_root='/home/aabdujyo/scratch/activitygraph_transformer/data/thumos', dataset='thumos', dec_layers=4, device='cpu', dim_feedforward=1024, dim_latent=128, dist_url='env://', dropout=0.0, enc_layers=4, eos_coef=0.1, epochs=10, eval=False, features='i3d_feats', hidden_dim=256, local_rank=None, lr=1e-05, lr_drop=1500, lr_joiner=1e-05, model='agt', nheads=4, norm_type='bn', num_classes=20, num_inputs=512, num_pos_embed_dict=512, num_queries=500, num_workers=0, output_dir='output/checkpoints_/checkpoints_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512', position_embedding='learned', pre_norm=False, resume='', sample_rate=4, save_checkpoint_every=30, seed=42, segment_loss_coef=5.0, set_cost_class=1, set_cost_segment=5.0, set_cost_siou=3.0, siou_loss_coef=3.0, start_epoch=0, step_size=1, weight_decay=0.0, world_size=1)
AGT(
  (transformer): Graph_Transformer(
    (encoder): GraphTransformerEncoder(
      (layers): ModuleList(
        (0): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (1): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (2): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (3): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (decoder): GraphTransformerDecoder(
      (layers): ModuleList(
        (0): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
        (1): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
        (2): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
        (3): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
      )
      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
  )
  (joiner): Joiner(
    (0): PositionEmbeddingLearned(
      (row_embed): Embedding(512, 256)
    )
  )
  (query_embed): Embedding(500, 256)
  (class_embed): Linear(in_features=256, out_features=21, bias=True)
  (segments_embed): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
      (2): Linear(in_features=256, out_features=2, bias=True)
    )
  )
  (input_proj): Conv1d(2048, 256, kernel_size=(1,), stride=(1,))
)
number of params: 10150935
> [0;32m/scratch/aabdujyo/activitygraph_transformer/src/datasets/thumos.py[0m(167)[0;36mbuild_thumos_detection[0;34m()[0m
[0;32m    166 [0;31m    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 167 [0;31m    [0mfeatures_path[0m [0;34m=[0m [0mos[0m[0;34m.[0m[0mpath[0m[0;34m.[0m[0mjoin[0m[0;34m([0m[0margs[0m[0;34m.[0m[0mdata_root[0m[0;34m,[0m [0margs[0m[0;34m.[0m[0mfeatures[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    168 [0;31m    [0mvid_data_file[0m [0;34m=[0m [0mos[0m[0;34m.[0m[0mpath[0m[0;34m.[0m[0mjoin[0m[0;34m([0m[0margs[0m[0;34m.[0m[0mdata_root[0m[0;34m,[0m [0margs[0m[0;34m.[0m[0mdataset[0m[0;34m+[0m [0;34m"_action.json"[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> *** SyntaxError: Missing parentheses in call to 'print'. Did you mean print(args.data_roor)?
ipdb> *** SyntaxError: invalid syntax
ipdb> mode = 'training'
args = Namespace(activation='leaky_relu', aux_loss=False, batch_size=4, clip_max_norm=0.0, cuda=False, data_root='/home/aabdujyo/scratch/activitygraph_transformer/data/thumos', dataset='thumos', dec_layers=4, device='cpu', dim_feedforward=1024, dim_latent=128, dist_url='env://', dropout=0.0, enc_layers=4, eos_coef=0.1, epochs=10, eval=False, features='i3d_feats', hidden_dim=256, local_rank=None, lr=1e-05, lr_drop=1500, lr_joiner=1e-05, model='agt', nheads=4, norm_type='bn', num_classes=20, num_inputs=512, num_pos_embed_dict=512, num_queries=500, num_workers=0, output_dir='output/checkpoints_/checkpoints_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512', position_embedding='learned', pre_norm=False, resume='', sample_rate=4, save_checkpoint_every=30, seed=42, segment_loss_coef=5.0, set_cost_class=1, set_cost_segment=5.0, set_cost_siou=3.0, siou_loss_coef=3.0, start_epoch=0, step_size=1, weight_decay=0.0, world_size=1)
ipdb> '/home/aabdujyo/scratch/activitygraph_transformer/data/thumos'
ipdb> 'i3d_feats'
ipdb> Exiting Debugger.
Logging output to output/logs/log_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512.log
Namespace(activation='leaky_relu', aux_loss=False, batch_size=4, clip_max_norm=0.0, cuda=False, data_root='/home/aabdujyo/scratch/activitygraph_transformer/data/thumos', dataset='thumos', dec_layers=4, device='cpu', dim_feedforward=1024, dim_latent=128, dist_url='env://', dropout=0.0, enc_layers=4, eos_coef=0.1, epochs=10, eval=False, features='i3d_feats', hidden_dim=256, local_rank=None, lr=1e-05, lr_drop=1500, lr_joiner=1e-05, model='agt', nheads=4, norm_type='bn', num_classes=20, num_inputs=512, num_pos_embed_dict=512, num_queries=500, num_workers=0, output_dir='output/checkpoints_/checkpoints_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512', position_embedding='learned', pre_norm=False, resume='', sample_rate=4, save_checkpoint_every=30, seed=42, segment_loss_coef=5.0, set_cost_class=1, set_cost_segment=5.0, set_cost_siou=3.0, siou_loss_coef=3.0, start_epoch=0, step_size=1, weight_decay=0.0, world_size=1)
AGT(
  (transformer): Graph_Transformer(
    (encoder): GraphTransformerEncoder(
      (layers): ModuleList(
        (0): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (1): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (2): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (3): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (decoder): GraphTransformerDecoder(
      (layers): ModuleList(
        (0): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
        (1): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
        (2): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
        (3): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
      )
      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
  )
  (joiner): Joiner(
    (0): PositionEmbeddingLearned(
      (row_embed): Embedding(512, 256)
    )
  )
  (query_embed): Embedding(500, 256)
  (class_embed): Linear(in_features=256, out_features=21, bias=True)
  (segments_embed): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
      (2): Linear(in_features=256, out_features=2, bias=True)
    )
  )
  (input_proj): Conv1d(2048, 256, kernel_size=(1,), stride=(1,))
)
number of params: 10150935
> [0;32m/scratch/aabdujyo/activitygraph_transformer/src/datasets/thumos.py[0m(169)[0;36mbuild_thumos_detection[0;34m()[0m
[0;32m    168 [0;31m    [0;31m# features_path = os.path.join(args.data_root, args.features)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 169 [0;31m    [0mfeatures_path[0m [0;34m=[0m [0mos[0m[0;34m.[0m[0mpath[0m[0;34m.[0m[0mjoin[0m[0;34m([0m[0margs[0m[0;34m.[0m[0mdata_root[0m[0;34m,[0m [0;34m"thumos_sr1/thumos/thumos_i3d_feats_sr1"[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    170 [0;31m    [0mvid_data_file[0m [0;34m=[0m [0mos[0m[0;34m.[0m[0mpath[0m[0;34m.[0m[0mjoin[0m[0;34m([0m[0margs[0m[0;34m.[0m[0mdata_root[0m[0;34m,[0m [0margs[0m[0;34m.[0m[0mdataset[0m[0;34m+[0m [0;34m"_action.json"[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> Exiting Debugger.
Logging output to output/logs/log_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512.log
Namespace(activation='leaky_relu', aux_loss=False, batch_size=4, clip_max_norm=0.0, cuda=False, data_root='/home/aabdujyo/scratch/activitygraph_transformer/data/thumos', dataset='thumos', dec_layers=4, device='cpu', dim_feedforward=1024, dim_latent=128, dist_url='env://', dropout=0.0, enc_layers=4, eos_coef=0.1, epochs=10, eval=False, features='i3d_feats', hidden_dim=256, local_rank=None, lr=1e-05, lr_drop=1500, lr_joiner=1e-05, model='agt', nheads=4, norm_type='bn', num_classes=20, num_inputs=512, num_pos_embed_dict=512, num_queries=500, num_workers=0, output_dir='output/checkpoints_/checkpoints_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512', position_embedding='learned', pre_norm=False, resume='', sample_rate=4, save_checkpoint_every=30, seed=42, segment_loss_coef=5.0, set_cost_class=1, set_cost_segment=5.0, set_cost_siou=3.0, siou_loss_coef=3.0, start_epoch=0, step_size=1, weight_decay=0.0, world_size=1)
AGT(
  (transformer): Graph_Transformer(
    (encoder): GraphTransformerEncoder(
      (layers): ModuleList(
        (0): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (1): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (2): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (3): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (decoder): GraphTransformerDecoder(
      (layers): ModuleList(
        (0): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
        (1): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
        (2): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
        (3): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
      )
      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
  )
  (joiner): Joiner(
    (0): PositionEmbeddingLearned(
      (row_embed): Embedding(512, 256)
    )
  )
  (query_embed): Embedding(500, 256)
  (class_embed): Linear(in_features=256, out_features=21, bias=True)
  (segments_embed): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
      (2): Linear(in_features=256, out_features=2, bias=True)
    )
  )
  (input_proj): Conv1d(2048, 256, kernel_size=(1,), stride=(1,))
)Logging output to output/logs/log_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512.log
Traceback (most recent call last):
  File "src/main.py", line 10, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'
Logging output to output/logs/log_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512.log
Namespace(activation='leaky_relu', aux_loss=False, batch_size=4, clip_max_norm=0.0, cuda=False, data_root='/home/aabdujyo/scratch/activitygraph_transformer/data/thumos', dataset='thumos', dec_layers=4, device='cpu', dim_feedforward=1024, dim_latent=128, dist_url='env://', dropout=0.0, enc_layers=4, eos_coef=0.1, epochs=10, eval=False, features='i3d_feats', hidden_dim=256, local_rank=None, lr=1e-05, lr_drop=1500, lr_joiner=1e-05, model='agt', nheads=4, norm_type='bn', num_classes=20, num_inputs=512, num_pos_embed_dict=512, num_queries=500, num_workers=0, output_dir='output/checkpoints_/checkpoints_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512', position_embedding='learned', pre_norm=False, resume='', sample_rate=4, save_checkpoint_every=30, seed=42, segment_loss_coef=5.0, set_cost_class=1, set_cost_segment=5.0, set_cost_siou=3.0, siou_loss_coef=3.0, start_epoch=0, step_size=1, weight_decay=0.0, world_size=1)
AGT(
  (transformer): Graph_Transformer(
    (encoder): GraphTransformerEncoder(
      (layers): ModuleList(
        (0): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (1): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (2): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (3): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (decoder): GraphTransformerDecoder(
      (layers): ModuleList(
        (0): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
        (1): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
        (2): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
        (3): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
      )
      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
  )
  (joiner): Joiner(
    (0): PositionEmbeddingLearned(
      (row_embed): Embedding(512, 256)
    )
  )
  (query_embed): Embedding(500, 256)
  (class_embed): Linear(in_features=256, out_features=21, bias=True)
  (segments_embed): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
      (2): Linear(in_features=256, out_features=2, bias=True)
    )
  )
  (input_proj): Conv1d(2048, 256, kernel_size=(1,), stride=(1,))
)Logging output to output/logs/log_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512.log
Namespace(activation='leaky_relu', aux_loss=False, batch_size=2, clip_max_norm=1, cuda=False, data_root=None, dataset='', dec_layers=3, device='cpu', dim_feedforward=2048, dim_latent=128, dist_url='env://', dropout=0.1, enc_layers=1, eos_coef=0.1, epochs=10, eval=False, features=None, hidden_dim=256, local_rank=None, lr=1e-05, lr_drop=100, lr_joiner=0, model='', nheads=8, norm_type='bn', num_classes=48, num_inputs=128, num_pos_embed_dict=256, num_queries=10, num_workers=0, output_dir='./checkpoints', position_embedding='learned', pre_norm=False, resume='', sample_rate=1, save_checkpoint_every=1000, seed=42, segment_loss_coef=5, set_cost_class=1, set_cost_segment=5, set_cost_siou=3, siou_loss_coef=3, start_epoch=0, step_size=64, weight_decay=1e-05, world_size=1)
Traceback (most recent call last):
  File "src/main.py", line 204, in <module>
    main(args)
  File "src/main.py", line 110, in main
    model, criterion, postprocessors = build_model(args)
  File "/scratch/aabdujyo/activitygraph_transformer/src/models/__init__.py", line 5, in build_model
    return build(args)
  File "/scratch/aabdujyo/activitygraph_transformer/src/models/agt.py", line 338, in build
    criterion.to(device)
UnboundLocalError: local variable 'device' referenced before assignment
run_scripts/run_agt_trainer.sh: line 47: --cuda: command not found
run_scripts/run_agt_trainer.sh: line 48: --dataset: command not found
Logging output to output/logs/log_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512.log
usage: main.py [-h] [--dataset DATASET] [--data_root DATA_ROOT]
               [--features FEATURES] [--output_dir OUTPUT_DIR]
               [--num_classes NUM_CLASSES] [--sample_rate SAMPLE_RATE]
               [--num_inputs NUM_INPUTS] [--model MODEL]
               [--num_queries NUM_QUERIES]
               [--num_pos_embed_dict NUM_POS_EMBED_DICT]
               [--dim_latent DIM_LATENT] [--hidden_dim HIDDEN_DIM]
               [--position_embedding POSITION_EMBEDDING] [--dropout DROPOUT]
               [--nheads NHEADS] [--dim_feedforward DIM_FEEDFORWARD]
               [--enc_layers ENC_LAYERS] [--dec_layers DEC_LAYERS]
               [--pre_norm] [--aux_loss] [--cuda] [--eval]
               [--norm_type {gn,bn}] [--activation ACTIVATION]
               [--set_cost_class SET_COST_CLASS]
               [--set_cost_segment SET_COST_SEGMENT]
               [--set_cost_siou SET_COST_SIOU]
               [--segment_loss_coef SEGMENT_LOSS_COEF]
               [--siou_loss_coef SIOU_LOSS_COEF] [--eos_coef EOS_COEF]
               [--resume RESUME]
               [--save_checkpoint_every SAVE_CHECKPOINT_EVERY]
               [--num_workers NUM_WORKERS] [--batch_size BATCH_SIZE]
               [--epochs EPOCHS] [--step_size STEP_SIZE]
               [--start_epoch START_EPOCH] [--seed SEED] [--lr LR]
               [--lr_joiner LR_JOINER] [--weight_decay WEIGHT_DECAY]
               [--lr_drop LR_DROP] [--clip_max_norm CLIP_MAX_NORM]
               [--world_size WORLD_SIZE] [--dist_url DIST_URL]
               [--local_rank LOCAL_RANK] [--device DEVICE]
main.py: error: unrecognized arguments:  # -m torch.distributed.launch  # --cluster
Logging output to output/logs/log_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512.log
Namespace(activation='leaky_relu', aux_loss=False, batch_size=4, clip_max_norm=0.0, cuda=True, data_root='/home/aabdujyo/scratch/activitygraph_transformer/data/thumos', dataset='thumos', dec_layers=4, device='cpu', dim_feedforward=1024, dim_latent=128, dist_url='env://', dropout=0.0, enc_layers=4, eos_coef=0.1, epochs=10, eval=False, features='i3d_feats', hidden_dim=256, local_rank=None, lr=1e-05, lr_drop=1500, lr_joiner=1e-05, model='agt', nheads=4, norm_type='bn', num_classes=20, num_inputs=512, num_pos_embed_dict=512, num_queries=500, num_workers=0, output_dir='output/checkpoints_/checkpoints_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512', position_embedding='learned', pre_norm=False, resume='', sample_rate=4, save_checkpoint_every=30, seed=42, segment_loss_coef=5.0, set_cost_class=1, set_cost_segment=5.0, set_cost_siou=3.0, siou_loss_coef=3.0, start_epoch=0, step_size=1, weight_decay=0.0, world_size=1)
| distributed init (rank 0): env://
Traceback (most recent call last):
  File "src/main.py", line 204, in <module>
    main(args)
  File "src/main.py", line 95, in main
    utils.init_distributed_mode(args)
  File "/scratch/aabdujyo/activitygraph_transformer/src/utils/misc.py", line 380, in init_distributed_mode
    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,world_size=args.world_size, rank=args.rank)
  File "/home/aabdujyo/softwares/anaconda3/envs/agt_env/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 397, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/home/aabdujyo/softwares/anaconda3/envs/agt_env/lib/python3.8/site-packages/torch/distributed/rendezvous.py", line 155, in _env_rendezvous_handler
    raise _env_error("MASTER_ADDR")
ValueError: Error initializing torch.distributed using env:// rendezvous: environment variable MASTER_ADDR expected, but not set
Logging output to output/logs/log_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512.log
usage: main.py [-h] [--dataset DATASET] [--data_root DATA_ROOT]
               [--features FEATURES] [--output_dir OUTPUT_DIR]
               [--num_classes NUM_CLASSES] [--sample_rate SAMPLE_RATE]
               [--num_inputs NUM_INPUTS] [--model MODEL]
               [--num_queries NUM_QUERIES]
               [--num_pos_embed_dict NUM_POS_EMBED_DICT]
               [--dim_latent DIM_LATENT] [--hidden_dim HIDDEN_DIM]
               [--position_embedding POSITION_EMBEDDING] [--dropout DROPOUT]
               [--nheads NHEADS] [--dim_feedforward DIM_FEEDFORWARD]
               [--enc_layers ENC_LAYERS] [--dec_layers DEC_LAYERS]
               [--pre_norm] [--aux_loss] [--cuda] [--eval]
               [--norm_type {gn,bn}] [--activation ACTIVATION]
               [--set_cost_class SET_COST_CLASS]
               [--set_cost_segment SET_COST_SEGMENT]
               [--set_cost_siou SET_COST_SIOU]
               [--segment_loss_coef SEGMENT_LOSS_COEF]
               [--siou_loss_coef SIOU_LOSS_COEF] [--eos_coef EOS_COEF]
               [--resume RESUME]
               [--save_checkpoint_every SAVE_CHECKPOINT_EVERY]
               [--num_workers NUM_WORKERS] [--batch_size BATCH_SIZE]
               [--epochs EPOCHS] [--step_size STEP_SIZE]
               [--start_epoch START_EPOCH] [--seed SEED] [--lr LR]
               [--lr_joiner LR_JOINER] [--weight_decay WEIGHT_DECAY]
               [--lr_drop LR_DROP] [--clip_max_norm CLIP_MAX_NORM]
               [--world_size WORLD_SIZE] [--dist_url DIST_URL]
               [--local_rank LOCAL_RANK] [--device DEVICE]
main.py: error: unrecognized arguments: -m torch.distributed.launch
Logging output to output/logs/log_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512.log
Namespace(activation='leaky_relu', aux_loss=False, batch_size=4, clip_max_norm=0.0, cuda=True, data_root='/home/aabdujyo/scratch/activitygraph_transformer/data/thumos', dataset='thumos', dec_layers=4, device='cpu', dim_feedforward=1024, dim_latent=128, dist_url='env://', dropout=0.0, enc_layers=4, eos_coef=0.1, epochs=10, eval=False, features='i3d_feats', hidden_dim=256, local_rank=None, lr=1e-05, lr_drop=1500, lr_joiner=1e-05, model='agt', nheads=4, norm_type='bn', num_classes=20, num_inputs=512, num_pos_embed_dict=512, num_queries=500, num_workers=0, output_dir='output/checkpoints_/checkpoints_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512', position_embedding='learned', pre_norm=False, resume='', sample_rate=4, save_checkpoint_every=30, seed=42, segment_loss_coef=5.0, set_cost_class=1, set_cost_segment=5.0, set_cost_siou=3.0, siou_loss_coef=3.0, start_epoch=0, step_size=1, weight_decay=0.0, world_size=1)
| distributed init (rank 0): env://
Traceback (most recent call last):
  File "src/main.py", line 204, in <module>
    main(args)
  File "src/main.py", line 95, in main
    utils.init_distributed_mode(args)
  File "/scratch/aabdujyo/activitygraph_transformer/src/utils/misc.py", line 380, in init_distributed_mode
    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,world_size=args.world_size, rank=args.rank)
  File "/home/aabdujyo/softwares/anaconda3/envs/agt_env/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 397, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/home/aabdujyo/softwares/anaconda3/envs/agt_env/lib/python3.8/site-packages/torch/distributed/rendezvous.py", line 155, in _env_rendezvous_handler
    raise _env_error("MASTER_ADDR")
ValueError: Error initializing torch.distributed using env:// rendezvous: environment variable MASTER_ADDR expected, but not set
Logging output to output/logs/log_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512.log
usage: main.py [-h] [--dataset DATASET] [--data_root DATA_ROOT]
               [--features FEATURES] [--output_dir OUTPUT_DIR]
               [--num_classes NUM_CLASSES] [--sample_rate SAMPLE_RATE]
               [--num_inputs NUM_INPUTS] [--model MODEL]
               [--num_queries NUM_QUERIES]
               [--num_pos_embed_dict NUM_POS_EMBED_DICT]
               [--dim_latent DIM_LATENT] [--hidden_dim HIDDEN_DIM]
               [--position_embedding POSITION_EMBEDDING] [--dropout DROPOUT]
               [--nheads NHEADS] [--dim_feedforward DIM_FEEDFORWARD]
               [--enc_layers ENC_LAYERS] [--dec_layers DEC_LAYERS]
               [--pre_norm] [--aux_loss] [--cuda] [--eval]
               [--norm_type {gn,bn}] [--activation ACTIVATION]
               [--set_cost_class SET_COST_CLASS]
               [--set_cost_segment SET_COST_SEGMENT]
               [--set_cost_siou SET_COST_SIOU]
               [--segment_loss_coef SEGMENT_LOSS_COEF]
               [--siou_loss_coef SIOU_LOSS_COEF] [--eos_coef EOS_COEF]
               [--resume RESUME]
               [--save_checkpoint_every SAVE_CHECKPOINT_EVERY]
               [--num_workers NUM_WORKERS] [--batch_size BATCH_SIZE]
               [--epochs EPOCHS] [--step_size STEP_SIZE]
               [--start_epoch START_EPOCH] [--seed SEED] [--lr LR]
               [--lr_joiner LR_JOINER] [--weight_decay WEIGHT_DECAY]
               [--lr_drop LR_DROP] [--clip_max_norm CLIP_MAX_NORM]
               [--world_size WORLD_SIZE] [--dist_url DIST_URL]
               [--local_rank LOCAL_RANK] [--device DEVICE]
main.py: error: unrecognized arguments: -m torch.distributed.launch
Logging output to output/logs/log_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512.log
Namespace(activation='leaky_relu', aux_loss=False, batch_size=4, clip_max_norm=0.0, cuda=True, data_root='/home/aabdujyo/scratch/activitygraph_transformer/data/thumos', dataset='thumos', dec_layers=4, device='cpu', dim_feedforward=1024, dim_latent=128, dist_url='env://', dropout=0.0, enc_layers=4, eos_coef=0.1, epochs=10, eval=False, features='i3d_feats', hidden_dim=256, local_rank=None, lr=1e-05, lr_drop=1500, lr_joiner=1e-05, model='agt', nheads=4, norm_type='bn', num_classes=20, num_inputs=512, num_pos_embed_dict=512, num_queries=500, num_workers=0, output_dir='output/checkpoints_/checkpoints_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512', position_embedding='learned', pre_norm=False, resume='', sample_rate=4, save_checkpoint_every=30, seed=42, segment_loss_coef=5.0, set_cost_class=1, set_cost_segment=5.0, set_cost_siou=3.0, siou_loss_coef=3.0, start_epoch=0, step_size=1, weight_decay=0.0, world_size=1)
| distributed init (rank 0): env://
Traceback (most recent call last):
  File "src/main.py", line 204, in <module>
    main(args)
  File "src/main.py", line 95, in main
    utils.init_distributed_mode(args)
  File "/scratch/aabdujyo/activitygraph_transformer/src/utils/misc.py", line 380, in init_distributed_mode
    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,world_size=args.world_size, rank=args.rank)
  File "/home/aabdujyo/softwares/anaconda3/envs/agt_env/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 397, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/home/aabdujyo/softwares/anaconda3/envs/agt_env/lib/python3.8/site-packages/torch/distributed/rendezvous.py", line 155, in _env_rendezvous_handler
    raise _env_error("MASTER_ADDR")
ValueError: Error initializing torch.distributed using env:// rendezvous: environment variable MASTER_ADDR expected, but not set
Logging output to output/logs/log_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512.log
Namespace(activation='leaky_relu', aux_loss=False, batch_size=4, clip_max_norm=0.0, cuda=True, data_root='/home/aabdujyo/scratch/activitygraph_transformer/data/thumos', dataset='thumos', dec_layers=4, device='cpu', dim_feedforward=1024, dim_latent=128, dist_url='env://', dropout=0.0, enc_layers=4, eos_coef=0.1, epochs=10, eval=False, features='i3d_feats', hidden_dim=256, local_rank=None, lr=1e-05, lr_drop=1500, lr_joiner=1e-05, model='agt', nheads=4, norm_type='bn', num_classes=20, num_inputs=512, num_pos_embed_dict=512, num_queries=500, num_workers=0, output_dir='output/checkpoints_/checkpoints_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512', position_embedding='learned', pre_norm=False, resume='', sample_rate=4, save_checkpoint_every=30, seed=42, segment_loss_coef=5.0, set_cost_class=1, set_cost_segment=5.0, set_cost_siou=3.0, siou_loss_coef=3.0, start_epoch=0, step_size=1, weight_decay=0.0, world_size=1)
> [0;32m/scratch/aabdujyo/activitygraph_transformer/src/main.py[0m(96)[0;36mmain[0;34m()[0m
[0;32m     95 [0;31m        [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 96 [0;31m        [0;32mif[0m [0mtorch[0m[0;34m.[0m[0mcuda[0m[0;34m.[0m[0mdevice_count[0m[0;34m([0m[0;34m)[0m [0;34m>=[0m [0;36m1[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     97 [0;31m            [0mutils[0m[0;34m.[0m[0minit_distributed_mode[0m[0;34m([0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> environ({'CPLUS_INCLUDE_PATH': '/cvmfs/soft.computecanada.ca/nix/var/nix/profiles/16.09/include', 'LMOD_FAMILY_COMPILER_VERSION': '2016.4', 'MKLROOT': '/cvmfs/restricted.computecanada.ca/easybuild/software/2017/Core/imkl/11.3.4.258/mkl', 'SLURM_NODELIST': 'cdr2603', 'MANPATH': '/cvmfs/soft.computecanada.ca/easybuild/software/2017/avx2/Compiler/intel2016.4/openmpi/2.1.1/share/man:/cvmfs/restricted.computecanada.ca/easybuild/software/2017/Core/ifort/2016.4.258/compilers_and_libraries_2016.4.258/linux/man/common:/cvmfs/restricted.computecanada.ca/easybuild/software/2017/Core/icc/2016.4.258/compilers_and_libraries_2016.4.258/linux/man/common:/cvmfs/soft.computecanada.ca/nix/var/nix/profiles/gcc-5.4.0/share/man:/cvmfs/soft.computecanada.ca/nix/var/nix/profiles/16.09/share/man:/cvmfs/soft.computecanada.ca/custom/software/lmod/lmod/share/man::/opt/puppetlabs/puppet/share/man', 'LMOD_AVAIL_EXTENSIONS': 'no', 'EBVERSIONOPENMPI': '2.1.1', 'OMPI_MCA_mtl': '^mxm', 'SLURM_JOB_NAME': 'interactive', 'XDG_SESSION_ID': '142144', 'EBROOTIMKL': '/cvmfs/restricted.computecanada.ca/easybuild/software/2017/Core/imkl/11.3.4.258', 'NIXUSER_PROFILE': '/cvmfs/soft.computecanada.ca/nix/var/nix/profiles/16.09', 'RSNT_ARCH': 'avx2', 'SLURMD_NODENAME': 'cdr2603', 'SLURM_TOPOLOGY_ADDR': 'core.leaf-f1-11.h2-6-1.cdr2603', 'TERM_PROGRAM': 'vscode', 'HOSTNAME': 'cedar1.cedar.computecanada.ca', '_ModuleTable003_': 'dCJdPTEsWyJzdGFja0RlcHRoIl09MixbInN0YXR1cyJdPSJhY3RpdmUiLFsidXNlck5hbWUiXT0iaWNjLy4yMDE2LjQuMjU4Iix9LGlmb3J0PXtbImZuIl09Ii9jdm1mcy9zb2Z0LmNvbXB1dGVjYW5hZGEuY2EvZWFzeWJ1aWxkL21vZHVsZXMvMjAxNy9Db3JlL2lmb3J0Ly4yMDE2LjQuMjU4Lmx1YSIsWyJmdWxsTmFtZSJdPSJpZm9ydC8uMjAxNi40LjI1OCIsWyJsb2FkT3JkZXIiXT01LHByb3BUPXt9LFsicmVmX2NvdW50Il09MSxbInN0YWNrRGVwdGgiXT0yLFsic3RhdHVzIl09ImFjdGl2ZSIsWyJ1c2VyTmFtZSJdPSJpZm9ydC8uMjAxNi40LjI1OCIsfSxpbWtsPXtbImZuIl09Ii9jdm1mcy9zb2Z0LmNvbXB1dGVjYW5hZGEuY2EvZWFzeWJ1aWxkL21vZHVsZXMvMjAxNy9D', 'SSL_CERT_FILE': '/etc/pki/tls/certs/ca-bundle.crt', 'SLURM_SRUN_COMM_PORT': '39066', 'INTEL_LICENSE_FILE': '/cvmfs/soft.computecanada.ca/config/licenses/intel/cedar.lic', 'RSNT_CUDA_DRIVER_VERSION': '455.45.01', '__LMOD_Priority_MODULEPATH': '/opt/software/modulefiles:-10', 'SLURM_NODE_ALIASES': '(null)', 'TERM': 'xterm-256color', 'SHELL': '/bin/bash', 'CC_RESTRICTED': 'true', 'EASYBUILD_BUILDPATH': '/tmp/aabdujyo', '__LMOD_REF_COUNT_MODULEPATH': '/opt/software/modulefiles:4;/cvmfs/soft.computecanada.ca/easybuild/modules/2017/avx2/MPI/intel2016.4/openmpi2.1:1;/cvmfs/soft.computecanada.ca/easybuild/modules/2017/avx2/Compiler/intel2016.4:1;/home/aabdujyo/.local/easybuild/modules/2017/Core:1;/cvmfs/soft.computecanada.ca/easybuild/modules/2017/Core:1;/cvmfs/soft.computecanada.ca/custom/modules:1', 'SLURM_PTY_WIN_ROW': '14', 'AMD_ENTRYPOINT': 'vs/server/remoteExtensionHostProcess', 'HISTSIZE': '1000', 'LMOD_ROOT': '/cvmfs/soft.computecanada.ca/custom/software/lmod', 'EBROOTGCCCORE': '/cvmfs/soft.computecanada.ca/nix/var/nix/profiles/gcc-5.4.0', 'PROJECT': '/home/aabdujyo/project', 'SLURM_MPI_TYPE': 'pmix_v1', 'SLURM_JOB_QOS': 'normal', 'SLURM_TOPOLOGY_ADDR_PATTERN': 'switch.switch.switch.node', 'SSH_CLIENT': '24.84.10.158 55932 22', 'TMPDIR': '/tmp', 'MODULEPATH_ROOT': '/cvmfs/soft.computecanada.ca/easybuild/modules', 'LMOD_SYSTEM_DEFAULT_MODULES': 'StdEnv', 'CURL_CA_BUNDLE': '/etc/pki/tls/certs/ca-bundle.crt', 'BASH_FUNC_ml()': '() {  eval $($LMOD_DIR/ml_cmd "$@")\n}', 'SLURM_JOB_CPUS_PER_NODE_PACK_GROUP_0': '1', 'LMOD_PACKAGE_PATH': '/cvmfs/soft.computecanada.ca/config/lmod/', 'LIBRARY_PATH': '/cvmfs/soft.computecanada.ca/easybuild/software/2017/avx2/Compiler/intel2016.4/openmpi/2.1.1/lib:/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/ifort/2016.4.258/compilers_and_libraries_2016.4.258/linux/compiler/lib/intel64:/cvmfs/restricted.computecanada.ca/easybuild/software/2017/Core/ifort/2016.4.258/compilers_and_libraries_2016.4.258/linux/compiler/lib/intel64:/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/icc/2016.4.258/compilers_and_libraries_2016.4.258/linux/compiler/lib/intel64:/cvmfs/restricted.computecanada.ca/easybuild/software/2017/Core/icc/2016.4.258/compilers_and_libraries_2016.4.258/linux/compiler/lib/intel64:/cvmfs/soft.computecanada.ca/nix/var/nix/profiles/gcc-5.4.0/lib64:/cvmfs/soft.computecanada.ca/nix/var/nix/profiles/gcc-5.4.0/lib:/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/imkl/11.3.4.258/mkl/lib/intel64:/cvmfs/restricted.computecanada.ca/easybuild/software/2017/Core/imkl/11.3.4.258/mkl/lib/intel64:/cvmfs/restricted.computecanada.ca/easybuild/software/2017/Core/imkl/11.3.4.258/lib/intel64:/cvmfs/soft.computecanada.ca/nix/var/nix/profiles/16.09/lib', 'PERL5LIB': '/cvmfs/soft.computecanada.ca/nix/var/nix/profiles/16.09/lib/perl5:/cvmfs/soft.computecanada.ca/nix/var/nix/profiles/16.09/lib/perl5/site_perl', 'CONDA_SHLVL': '2', 'TERM_PROGRAM_VERSION': '1.41.1', 'MKL_ENABLE_INSTRUCTIONS': 'AVX512', 'CONDA_PROMPT_MODIFIER': '(agt_env) ', 'GSETTINGS_SCHEMA_DIR_CONDA_BACKUP': '', 'SLURM_CPU_BIND_VERBOSE': 'quiet', 'FPATH': '/cvmfs/soft.computecanada.ca/custom/software/lmod/lmod/init/ksh_funcs', 'LMOD_PKG': '/cvmfs/soft.computecanada.ca/custom/software/lmod/lmod', 'LMOD_VERSION': '8.4.16', 'LMOD_ADMIN_FILE': '/cvmfs/soft.computecanada.ca/config/lmod/admin.list', 'LMOD_SHORT_TIME': '3600', 'CMAKE_LIBRARY_PATH': '/cvmfs/soft.computecanada.ca/nix/var/nix/profiles/gcc-5.4.0/lib64', 'LMOD_FAMILY_BASE_OS': 'nixpkgs', 'MIC_LD_LIBRARY_PATH': '/cvmfs/restricted.computecanada.ca/easybuild/software/2017/Core/imkl/11.3.4.258/mkl/lib/mic:/cvmfs/restricted.computecanada.ca/easybuild/software/2017/Core/imkl/11.3.4.258/lib/intel64_lin_mic', 'PYTHONBREAKPOINT': 'ipdb.set_trace', 'SLURM_CPU_BIND_LIST': '0x00000040', '__LMOD_REF_COUNT_LOADEDMODULES': 'nixpkgs/16.09:1;imkl/11.3.4.258:1;gcccore/.5.4.0:1;icc/.2016.4.258:1;ifort/.2016.4.258:1;intel/2016.4:1;openmpi/2.1.1:1;StdEnv/2016.4:1', 'OMPI_MCA_pml': '^yalla,ucx', '_ModuleTable007_': 'cHV0ZWNhbmFkYS5jYS9lYXN5YnVpbGQvbW9kdWxlcy8yMDE3L0NvcmUiLCIvY3ZtZnMvc29mdC5jb21wdXRlY2FuYWRhLmNhL2N1c3RvbS9tb2R1bGVzIix9LFsic3lzdGVtQmFzZU1QQVRIIl09Ii9jdm1mcy9zb2Z0LmNvbXB1dGVjYW5hZGEuY2EvY3VzdG9tL21vZHVsZXMiLH0=', 'USER': 'aabdujyo', 'RSNT_SLURM_MPI_TYPE': 'pmix_v1', 'SLURM_NNODES': '1', 'LMOD_sys': 'Linux', 'FONTCONFIG_FILE': '/cvmfs/soft.computecanada.ca/nix/var/nix/profiles/16.09/etc/fonts/fonts.conf', 'LD_LIBRARY_PATH': '/opt/software/slurm/lib', 'LS_COLORS': 'rs=0:di=38;5;27:ln=38;5;51:mh=44;38;5;15:pi=40;38;5;11:so=38;5;13:do=38;5;5:bd=48;5;232;38;5;11:cd=48;5;232;38;5;3:or=48;5;232;38;5;9:mi=05;48;5;232;38;5;15:su=48;5;196;38;5;15:sg=48;5;11;38;5;16:ca=48;5;196;38;5;226:tw=48;5;10;38;5;16:ow=48;5;10;38;5;21:st=48;5;21;38;5;15:ex=38;5;34:*.tar=38;5;9:*.tgz=38;5;9:*.arc=38;5;9:*.arj=38;5;9:*.taz=38;5;9:*.lha=38;5;9:*.lz4=38;5;9:*.lzh=38;5;9:*.lzma=38;5;9:*.tlz=38;5;9:*.txz=38;5;9:*.tzo=38;5;9:*.t7z=38;5;9:*.zip=38;5;9:*.z=38;5;9:*.Z=38;5;9:*.dz=38;5;9:*.gz=38;5;9:*.lrz=38;5;9:*.lz=38;5;9:*.lzo=38;5;9:*.xz=38;5;9:*.bz2=38;5;9:*.bz=38;5;9:*.tbz=38;5;9:*.tbz2=38;5;9:*.tz=38;5;9:*.deb=38;5;9:*.rpm=38;5;9:*.jar=38;5;9:*.war=38;5;9:*.ear=38;5;9:*.sar=38;5;9:*.rar=38;5;9:*.alz=38;5;9:*.ace=38;5;9:*.zoo=38;5;9:*.cpio=38;5;9:*.7z=38;5;9:*.rz=38;5;9:*.cab=38;5;9:*.jpg=38;5;13:*.jpeg=38;5;13:*.gif=38;5;13:*.bmp=38;5;13:*.pbm=38;5;13:*.pgm=38;5;13:*.ppm=38;5;13:*.tga=38;5;13:*.xbm=38;5;13:*.xpm=38;5;13:*.tif=38;5;13:*.tiff=38;5;13:*.png=38;5;13:*.svg=38;5;13:*.svgz=38;5;13:*.mng=38;5;13:*.pcx=38;5;13:*.mov=38;5;13:*.mpg=38;5;13:*.mpeg=38;5;13:*.m2v=38;5;13:*.mkv=38;5;13:*.webm=38;5;13:*.ogm=38;5;13:*.mp4=38;5;13:*.m4v=38;5;13:*.mp4v=38;5;13:*.vob=38;5;13:*.qt=38;5;13:*.nuv=38;5;13:*.wmv=38;5;13:*.asf=38;5;13:*.rm=38;5;13:*.rmvb=38;5;13:*.flc=38;5;13:*.avi=38;5;13:*.fli=38;5;13:*.flv=38;5;13:*.gl=38;5;13:*.dl=38;5;13:*.xcf=38;5;13:*.xwd=38;5;13:*.yuv=38;5;13:*.cgm=38;5;13:*.emf=38;5;13:*.axv=38;5;13:*.anx=38;5;13:*.ogv=38;5;13:*.ogx=38;5;13:*.aac=38;5;45:*.au=38;5;45:*.flac=38;5;45:*.mid=38;5;45:*.midi=38;5;45:*.mka=38;5;45:*.mp3=38;5;45:*.mpc=38;5;45:*.ogg=38;5;45:*.ra=38;5;45:*.wav=38;5;45:*.axa=38;5;45:*.oga=38;5;45:*.spx=38;5;45:*.xspf=38;5;45:', 'CONDA_EXE': '/home/aabdujyo/softwares/anaconda3/bin/conda', 'SLURM_STEP_NUM_NODES': '1', 'EBVERSIONNIXPKGS': '16.09', 'SRUN_DEBUG': '3', 'GLOBUS_HOSTNAME': '206.12.124.2', 'CPATH': '/cvmfs/soft.computecanada.ca/easybuild/software/2017/avx2/Compiler/intel2016.4/openmpi/2.1.1/include:/cvmfs/restricted.computecanada.ca/easybuild/software/2017/Core/ifort/2016.4.258/include:/cvmfs/restricted.computecanada.ca/easybuild/software/2017/Core/imkl/11.3.4.258/mkl/include/fftw:/cvmfs/restricted.computecanada.ca/easybuild/software/2017/Core/imkl/11.3.4.258/mkl/include', '_ModuleTable004_': 'b3JlL2lta2wvMTEuMy40LjI1OC5sdWEiLFsiZnVsbE5hbWUiXT0iaW1rbC8xMS4zLjQuMjU4IixbImxvYWRPcmRlciJdPTIscHJvcFQ9e3R5cGVfPXtbIm1hdGgiXT0xLH0sfSxbInN0YWNrRGVwdGgiXT0xLFsic3RhdHVzIl09ImFjdGl2ZSIsWyJ1c2VyTmFtZSJdPSJpbWtsLzExLjMuNC4yNTgiLH0saW50ZWw9e1siZm4iXT0iL2N2bWZzL3NvZnQuY29tcHV0ZWNhbmFkYS5jYS9lYXN5YnVpbGQvbW9kdWxlcy8yMDE3L0NvcmUvaW50ZWwvMjAxNi40Lmx1YSIsWyJmdWxsTmFtZSJdPSJpbnRlbC8yMDE2LjQiLFsibG9hZE9yZGVyIl09Nixwcm9wVD17dHlwZV89e1sidG9vbHMiXT0xLH0sfSxbInN0YWNrRGVwdGgiXT0xLFsic3RhdHVzIl09ImFjdGl2ZSIsWyJ1c2VyTmFtZSJd', 'SLURM_JOBID': '61156975', 'TMOUT': '172800', '__LMOD_REF_COUNT__LMFILES_': '/cvmfs/soft.computecanada.ca/custom/modules/nixpkgs/16.09.lua:1;/cvmfs/soft.computecanada.ca/easybuild/modules/2017/Core/imkl/11.3.4.258.lua:1;/cvmfs/soft.computecanada.ca/easybuild/modules/2017/Core/gcccore/.5.4.0.lua:1;/cvmfs/soft.computecanada.ca/easybuild/modules/2017/Core/icc/.2016.4.258.lua:1;/cvmfs/soft.computecanada.ca/easybuild/modules/2017/Core/ifort/.2016.4.258.lua:1;/cvmfs/soft.computecanada.ca/easybuild/modules/2017/Core/intel/2016.4.lua:1;/cvmfs/soft.computecanada.ca/easybuild/modules/2017/avx2/Compiler/intel2016.4/openmpi/2.1.1.lua:1;/cvmfs/soft.computecanada.ca/custom/modules/StdEnv/2016.4.lua:1', 'OMPI_MCA_oob': '^ud', 'SLURM_LAUNCH_NODE_IPADDR': '172.19.128.2', 'PAGER': 'less -R', 'MKL_INTERFACE_LAYER': 'LP64,GNU', 'SLURM_STEP_ID': '4294967290', 'EBDEVELICCIFORT': '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/intel/2016.4/easybuild/Core-intel-2016.4-easybuild-devel', '_CE_CONDA': '', 'EBVERSIONIFORT': '2016.4.258', 'LMOD_FAMILY_MPI_VERSION': '2.1.1', 'XDG_CONFIG_DIRS': '/cvmfs/soft.computecanada.ca/nix/var/nix/profiles/16.09/etc/xdg:/etc/xdg', 'CONDA_PREFIX_1': '/home/aabdujyo/softwares/anaconda3', 'SACCT_FORMAT': 'Account,User,JobID,Start,End,AllocCPUS,Elapsed,AllocTRES%30,CPUTime,AveRSS,MaxRSS,MaxRSSTask,MaxRSSNode,NodeList,ExitCode,State%20', 'SLURM_RESTART_COUNT': '0', 'SLURM_STEP_LAUNCHER_PORT': '39066', 'PATH': '/opt/software/bin:/opt/software/slurm/bin:/opt/software/bin:/opt/software/slurm/bin:/home/aabdujyo/.vscode-server/bin/26076a4de974ead31f97692a0d32f90d735645c0/bin:/opt/software/bin:/opt/software/slurm/bin:/home/aabdujyo/.vscode-server/bin/26076a4de974ead31f97692a0d32f90d735645c0/bin:/home/aabdujyo/softwares/anaconda3/envs/agt_env/bin:/home/aabdujyo/softwares/anaconda3/condabin:/cvmfs/soft.computecanada.ca/easybuild/software/2017/avx2/Compiler/intel2016.4/openmpi/2.1.1/bin:/cvmfs/restricted.computecanada.ca/easybuild/software/2017/Core/ifort/2016.4.258/compilers_and_libraries_2016.4.258/linux/bin/intel64:/cvmfs/restricted.computecanada.ca/easybuild/software/2017/Core/icc/2016.4.258/compilers_and_libraries_2016.4.258/linux/bin/intel64:/cvmfs/soft.computecanada.ca/nix/var/nix/profiles/gcc-5.4.0/bin:/cvmfs/restricted.computecanada.ca/easybuild/software/2017/Core/imkl/11.3.4.258/mkl/bin:/cvmfs/restricted.computecanada.ca/easybuild/software/2017/Core/imkl/11.3.4.258/bin:/cvmfs/soft.computecanada.ca/custom/bin/computecanada:/cvmfs/soft.computecanada.ca/easybuild/bin:/cvmfs/soft.computecanada.ca/custom/bin:/cvmfs/soft.computecanada.ca/nix/var/nix/profiles/16.09/bin:/cvmfs/soft.computecanada.ca/nix/var/nix/profiles/16.09/sbin:/opt/software/bin:/opt/software/slurm/bin:/usr/local/bin:/usr/bin:/opt/puppetlabs/bin:/opt/dell/srvadmin/bin:/usr/local/sbin:/usr/sbin:/home/aabdujyo/.local/bin:/home/aabdujyo/bin', 'MAIL': '/var/spool/mail/aabdujyo', 'PIP_CONFIG_FILE': '/cvmfs/soft.computecanada.ca/config/python/pip-avx2.conf', '_ModuleTable001_': 'X01vZHVsZVRhYmxlXz17WyJNVHZlcnNpb24iXT0zLFsiY19yZWJ1aWxkVGltZSJdPWZhbHNlLFsiY19zaG9ydFRpbWUiXT1mYWxzZSxkZXB0aFQ9e30sZmFtaWx5PXtbImJhc2Vfb3MiXT0ibml4cGtncyIsWyJjb21waWxlciJdPSJpbnRlbCIsWyJtcGkiXT0ib3Blbm1waSIsfSxtVD17U3RkRW52PXtbImZuIl09Ii9jdm1mcy9zb2Z0LmNvbXB1dGVjYW5hZGEuY2EvY3VzdG9tL21vZHVsZXMvU3RkRW52LzIwMTYuNC5sdWEiLFsiZnVsbE5hbWUiXT0iU3RkRW52LzIwMTYuNCIsWyJsb2FkT3JkZXIiXT04LHByb3BUPXtsbW9kPXtbInN0aWNreSJdPTEsfSx9LFsic3RhY2tEZXB0aCJdPTAsWyJzdGF0dXMiXT0iYWN0aXZlIixbInVzZXJOYW1lIl09IlN0ZEVudiIsfSxnY2Njb3Jl', '__LMOD_STACK_LESSOPEN': 'fC9jdm1mcy9zb2Z0LmNvbXB1dGVjYW5hZGEuY2Evbml4L3Zhci9uaXgvcHJvZmlsZXMvMTYuMDkvYmluL2xlc3NwaXBlLnNoICVz:fHwvdXNyL2Jpbi9sZXNzcGlwZS5zaCAlcw==', 'SLURM_TASKS_PER_NODE': '1', 'GSETTINGS_SCHEMA_DIR': '/home/aabdujyo/softwares/anaconda3/envs/agt_env/share/glib-2.0/schemas', 'SLURM_WORKING_CLUSTER': 'cedar:172.16.128.61:6817:9216:109', 'EBDEVELIMKL': '/cvmfs/restricted.computecanada.ca/easybuild/software/2017/Core/imkl/11.3.4.258/easybuild/Core-imkl-11.3.4.258-easybuild-devel', 'RSNT_INTERCONNECT': 'omnipath', 'SLURM_CONF': '/etc/slurm/slurm.conf', 'CONDA_MKL_INTERFACE_LAYER_BACKUP': '', 'SQUEUE_SORT': '-t,e,S', 'SLURM_JOB_ID': '61156975', 'SLURM_PMIX_MAPPING_SERV': '(vector,(0,1,1))', 'CC_JOBSTAT': '61156975', 'CC_PROJECT': '/home/aabdujyo/project', 'C_INCLUDE_PATH': '/cvmfs/soft.computecanada.ca/nix/var/nix/profiles/16.09/include', 'EBDEVELICC': '/cvmfs/restricted.computecanada.ca/easybuild/software/2017/Core/icc/2016.4.258/easybuild/Core-icc-.2016.4.258-easybuild-devel', 'CONDA_PREFIX': '/home/aabdujyo/softwares/anaconda3/envs/agt_env', 'SLURM_STEPID': '4294967290', 'SLURM_JOB_USER': 'aabdujyo', 'PWD': '/home/aabdujyo/scratch/activitygraph_transformer', 'SLURM_SRUN_COMM_HOST': '172.19.128.2', 'MKL_EXAMPLES': '/cvmfs/restricted.computecanada.ca/easybuild/software/2017/Core/imkl/11.3.4.258/mkl/examples/', '_LMFILES_': '/cvmfs/soft.computecanada.ca/custom/modules/nixpkgs/16.09.lua:/cvmfs/soft.computecanada.ca/easybuild/modules/2017/Core/imkl/11.3.4.258.lua:/cvmfs/soft.computecanada.ca/easybuild/modules/2017/Core/gcccore/.5.4.0.lua:/cvmfs/soft.computecanada.ca/easybuild/modules/2017/Core/icc/.2016.4.258.lua:/cvmfs/soft.computecanada.ca/easybuild/modules/2017/Core/ifort/.2016.4.258.lua:/cvmfs/soft.computecanada.ca/easybuild/modules/2017/Core/intel/2016.4.lua:/cvmfs/soft.computecanada.ca/easybuild/modules/2017/avx2/Compiler/intel2016.4/openmpi/2.1.1.lua:/cvmfs/soft.computecanada.ca/custom/modules/StdEnv/2016.4.lua', 'CONDA_PYTHONBREAKPOINT': '', 'SLURM_CPU_BIND_TYPE': 'mask_cpu:', 'EBDEVELGCCCORE': '/cvmfs/soft.computecanada.ca/nix/var/nix/profiles/gcc-5.4.0/easybuild/Core-gcccore-.5.4.0-easybuild-devel', 'TZDIR': '/cvmfs/soft.computecanada.ca/nix/var/nix/profiles/16.09/share/zoneinfo', 'LANG': 'en_US.UTF-8', 'EBVERSIONICCIFORT': '2016.4', 'SLURM_PTY_WIN_COL': '133', 'MODULEPATH': '/opt/software/modulefiles:/cvmfs/soft.computecanada.ca/easybuild/modules/2017/avx2/MPI/intel2016.4/openmpi2.1:/cvmfs/soft.computecanada.ca/easybuild/modules/2017/avx2/Compiler/intel2016.4:/home/aabdujyo/.local/easybuild/modules/2017/Core:/cvmfs/soft.computecanada.ca/easybuild/modules/2017/Core:/cvmfs/soft.computecanada.ca/custom/modules', 'SLURM_JOB_UID': '3058393', 'EBROOTICCIFORT': '/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/intel/2016.4', 'LOADEDMODULES': 'nixpkgs/16.09:imkl/11.3.4.258:gcccore/.5.4.0:icc/.2016.4.258:ifort/.2016.4.258:intel/2016.4:openmpi/2.1.1:StdEnv/2016.4', '_ModuleTable_Sz_': '7', 'SLURM_NODEID': '0', 'MYPROXY_SERVER': 'myproxy.westgrid.ca', 'RSNT_ENABLE_STDENV2020_TRANSITION': 'no', 'SLURM_SUBMIT_DIR': '/scratch/aabdujyo/activitygraph_transformer', 'SLURM_TASK_PID': '80468', 'LMOD_CMD': '/cvmfs/soft.computecanada.ca/custom/software/lmod/lmod/libexec/lmod', 'EBROOTIFORT': '/cvmfs/restricted.computecanada.ca/easybuild/software/2017/Core/ifort/2016.4.258', '_ModuleTable005_': 'PSJpbnRlbC8yMDE2LjQiLH0sbml4cGtncz17WyJmbiJdPSIvY3ZtZnMvc29mdC5jb21wdXRlY2FuYWRhLmNhL2N1c3RvbS9tb2R1bGVzL25peHBrZ3MvMTYuMDkubHVhIixbImZ1bGxOYW1lIl09Im5peHBrZ3MvMTYuMDkiLFsibG9hZE9yZGVyIl09MSxwcm9wVD17bG1vZD17WyJzdGlja3kiXT0xLH0sfSxbInN0YWNrRGVwdGgiXT0xLFsic3RhdHVzIl09ImFjdGl2ZSIsWyJ1c2VyTmFtZSJdPSJuaXhwa2dzLzE2LjA5Iix9LG9wZW5tcGk9e1siZm4iXT0iL2N2bWZzL3NvZnQuY29tcHV0ZWNhbmFkYS5jYS9lYXN5YnVpbGQvbW9kdWxlcy8yMDE3L2F2eDIvQ29tcGlsZXIvaW50ZWwyMDE2LjQvb3Blbm1waS8yLjEuMS5sdWEiLFsiZnVsbE5hbWUiXT0ib3Blbm1waS8yLjEuMSIs', 'SLURM_CPUS_ON_NODE': '1', 'LMOD_AVAIL_STYLE': 'grouped:system', 'SQUEUE_FORMAT': '%.15i %.8u %.12a %.14j %.3t %.10L %.5D %.4C %.10b %.7m %N (%r) ', 'SLURM_PROCID': '0', '_CE_M': '', 'HISTCONTROL': 'ignoredups', 'LMOD_FAMILY_BASE_OS_VERSION': '16.09', 'SLURM_JOB_NODELIST': 'cdr2603', 'SLURM_PTY_PORT': '41266', 'HOME': '/home/aabdujyo', 'SHLVL': '9', '__LMOD_SET_FPATH': '1', 'EBVERSIONGCCCORE': '5.4.0', 'SLURM_LOCALID': '0', 'APPLICATION_INSIGHTS_NO_DIAGNOSTIC_CHANNEL': 'true', 'PIPE_LOGGING': 'true', 'GLOBUS_TCP_PORT_RANGE': '50000,51000', 'SLURM_JOB_GID': '3058393', '_ModuleTable002_': 'PXtbImZuIl09Ii9jdm1mcy9zb2Z0LmNvbXB1dGVjYW5hZGEuY2EvZWFzeWJ1aWxkL21vZHVsZXMvMjAxNy9Db3JlL2djY2NvcmUvLjUuNC4wLmx1YSIsWyJmdWxsTmFtZSJdPSJnY2Njb3JlLy41LjQuMCIsWyJsb2FkT3JkZXIiXT0zLHByb3BUPXt9LFsicmVmX2NvdW50Il09MixbInN0YWNrRGVwdGgiXT0zLFsic3RhdHVzIl09ImFjdGl2ZSIsWyJ1c2VyTmFtZSJdPSJnY2Njb3JlLy41LjQuMCIsfSxpY2M9e1siZm4iXT0iL2N2bWZzL3NvZnQuY29tcHV0ZWNhbmFkYS5jYS9lYXN5YnVpbGQvbW9kdWxlcy8yMDE3L0NvcmUvaWNjLy4yMDE2LjQuMjU4Lmx1YSIsWyJmdWxsTmFtZSJdPSJpY2MvLjIwMTYuNC4yNTgiLFsibG9hZE9yZGVyIl09NCxwcm9wVD17fSxbInJlZl9jb3Vu', 'SLURM_JOB_CPUS_PER_NODE': '1', 'SLURM_CLUSTER_NAME': 'cedar', 'SLURM_GTIDS': '0', 'SLURM_SUBMIT_HOST': 'cedar1.cedar.computecanada.ca', 'BASH_ENV': '/cvmfs/soft.computecanada.ca/custom/software/lmod/lmod/init/bash', 'SLURM_JOB_PARTITION': 'gpubase_bygpu_b1,gpubackfill', 'LOGNAME': 'aabdujyo', 'MODULERCFILE': ':/cvmfs/soft.computecanada.ca/config/lmod//modulerc', 'EBROOTOPENMPI': '/cvmfs/soft.computecanada.ca/easybuild/software/2017/avx2/Compiler/intel2016.4/openmpi/2.1.1', 'OMPI_MCA_plm_slurm_args': '--whole', 'PYTHONPATH': '/cvmfs/soft.computecanada.ca/custom/python/site-packages', 'CONDA_PYTHON_EXE': '/home/aabdujyo/softwares/anaconda3/bin/python', 'SLURM_TMPDIR': '/localscratch/aabdujyo.61156975.0', 'SLURM_STEP_NUM_TASKS': '1', 'GLOBUS_TCP_SOURCE_RANGE': '50000,51000', 'EASYBUILD_CONFIGFILES': '/cvmfs/soft.computecanada.ca/easybuild/config.cfg', 'SSH_CONNECTION': '24.84.10.158 55932 206.12.124.2 22', 'EBDEVELIFORT': '/cvmfs/restricted.computecanada.ca/easybuild/software/2017/Core/ifort/2016.4.258/easybuild/Core-ifort-.2016.4.258-easybuild-devel', 'LOCALE_ARCHIVE': '/cvmfs/soft.computecanada.ca/nix/var/nix/profiles/16.09/lib/locale/locale-archive', 'XDG_DATA_DIRS': '/cvmfs/soft.computecanada.ca/nix/var/nix/profiles/16.09/share:/usr/local/share:/usr/share', 'SLURM_JOB_ACCOUNT': 'rrg-mori_gpu', 'VSCODE_IPC_HOOK_CLI': '/tmp/vscode-ipc-0866cdf0-7baa-471d-aac4-09a5efa1bff7.sock', 'MODULESHOME': '/cvmfs/soft.computecanada.ca/custom/software/lmod/lmod', 'LMOD_RC': '/cvmfs/soft.computecanada.ca/config/lmod//lmodrc.lua', 'GXX_ROOT': '/cvmfs/soft.computecanada.ca/nix/var/nix/profiles/gcc-5.4.0/lib/gcc/x86_64-unknown-linux-gnu/5.4.0', 'SLURM_JOB_NUM_NODES': '1', 'LESSOPEN': '|/cvmfs/soft.computecanada.ca/nix/var/nix/profiles/16.09/bin/lesspipe.sh %s', 'LMOD_SETTARG_FULL_SUPPORT': 'no', 'EBDEVELOPENMPI': '/cvmfs/soft.computecanada.ca/easybuild/software/2017/avx2/Compiler/intel2016.4/openmpi/2.1.1/easybuild/avx2-Compiler-intel2016.4-openmpi-2.1.1-easybuild-devel', 'PKG_CONFIG_PATH': '/cvmfs/soft.computecanada.ca/easybuild/software/2017/avx2/Compiler/intel2016.4/openmpi/2.1.1/lib/pkgconfig:/cvmfs/soft.computecanada.ca/nix/var/nix/profiles/16.09/lib/pkgconfig:/cvmfs/soft.computecanada.ca/nix/var/nix/profiles/16.09/share/pkgconfig', 'CONDA_DEFAULT_ENV': 'agt_env', 'SLURM_STEP_TASKS_PER_NODE': '1', 'ARCH': 'x86_64', '__Init_Default_Modules': '1', 'EBVERSIONIMKL': '11.3.4.258', 'SLURM_STEP_NODELIST': 'cdr2603', 'CC_CLUSTER': 'cedar', 'ACLOCAL_PATH': '/cvmfs/soft.computecanada.ca/nix/var/nix/profiles/16.09/share/aclocal', 'LMOD_FAMILY_COMPILER': 'intel', 'XDG_RUNTIME_DIR': '/run/user/3058393', 'CMAKE_PREFIX_PATH': '/cvmfs/soft.computecanada.ca/easybuild/software/2017/avx2/Compiler/intel2016.4/openmpi/2.1.1:/cvmfs/soft.computecanada.ca/nix/var/nix/profiles/gcc-5.4.0', 'EBVERSIONICC': '2016.4.258', 'OMPI_MCA_coll': '^fca,hcoll', 'BASH_FUNC_module()': '() {  eval $($LMOD_CMD bash "$@") && eval $(${LMOD_SETTARG_CMD:-:} -s sh)\n}', 'SLURM_CPU_BIND': 'quiet,mask_cpu:0x00000040', 'VERBOSE_LOGGING': 'true', 'LMOD_DIR': '/cvmfs/soft.computecanada.ca/custom/software/lmod/lmod/libexec', 'EBROOTICC': '/cvmfs/restricted.computecanada.ca/easybuild/software/2017/Core/icc/2016.4.258', 'RSNT_LD_LIBRARY_PATH': '/opt/software/slurm/lib', '_ModuleTable006_': 'WyJsb2FkT3JkZXIiXT03LHByb3BUPXt0eXBlXz17WyJtcGkiXT0xLH0sfSxbInN0YWNrRGVwdGgiXT0xLFsic3RhdHVzIl09ImFjdGl2ZSIsWyJ1c2VyTmFtZSJdPSJvcGVubXBpLzIuMS4xIix9LH0sbXBhdGhBPXsiL29wdC9zb2Z0d2FyZS9tb2R1bGVmaWxlcyIsIi9jdm1mcy9zb2Z0LmNvbXB1dGVjYW5hZGEuY2EvZWFzeWJ1aWxkL21vZHVsZXMvMjAxNy9hdngyL01QSS9pbnRlbDIwMTYuNC9vcGVubXBpMi4xIiwiL2N2bWZzL3NvZnQuY29tcHV0ZWNhbmFkYS5jYS9lYXN5YnVpbGQvbW9kdWxlcy8yMDE3L2F2eDIvQ29tcGlsZXIvaW50ZWwyMDE2LjQiLCIvaG9tZS9hYWJkdWp5by8ubG9jYWwvZWFzeWJ1aWxkL21vZHVsZXMvMjAxNy9Db3JlIiwiL2N2bWZzL3NvZnQuY29t', 'LMOD_FAMILY_MPI': 'openmpi', 'OMPI_MCA_btl': '^openib', 'SCRATCH': '/scratch/aabdujyo', 'SLURM_MEM_PER_NODE': '51200', 'COLORTERM': 'truecolor', 'EBROOTNIXPKGS': '/cvmfs/soft.computecanada.ca/nix/var/nix/profiles/16.09', 'PERL5OPT': '-I/cvmfs/soft.computecanada.ca/nix/var/nix/profiles/16.09/lib/perl5 -I/cvmfs/soft.computecanada.ca/nix/var/nix/profiles/16.09/lib/perl5/site_perl', 'BASH_FUNC_module%%': '() {  eval $($LMOD_CMD bash "$@") && eval $(${LMOD_SETTARG_CMD:-:} -s sh)\n}', 'BASH_FUNC_ml%%': '() {  eval $($LMOD_DIR/ml_cmd "$@")\n}', '_': '/home/aabdujyo/softwares/anaconda3/envs/agt_env/bin/python', 'KMP_INIT_AT_FORK': 'FALSE'})
ipdb> --Call--
> [0;32m/home/aabdujyo/softwares/anaconda3/envs/agt_env/lib/python3.8/site-packages/torch/cuda/__init__.py[0m(367)[0;36mdevice_count[0;34m()[0m
[0;32m    366 [0;31m[0;34m[0m[0m
[0m[0;32m--> 367 [0;31m[0;32mdef[0m [0mdevice_count[0m[0;34m([0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    368 [0;31m    [0;34mr"""Returns the number of GPUs available."""[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/aabdujyo/softwares/anaconda3/envs/agt_env/lib/python3.8/site-packages/torch/cuda/__init__.py[0m(369)[0;36mdevice_count[0;34m()[0m
[0;32m    368 [0;31m    [0;34mr"""Returns the number of GPUs available."""[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 369 [0;31m    [0;32mif[0m [0mis_available[0m[0;34m([0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    370 [0;31m        [0;32mreturn[0m [0mtorch[0m[0;34m.[0m[0m_C[0m[0;34m.[0m[0m_cuda_getDeviceCount[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/home/aabdujyo/softwares/anaconda3/envs/agt_env/lib/python3.8/site-packages/torch/cuda/__init__.py[0m(370)[0;36mdevice_count[0;34m()[0m
[0;32m    369 [0;31m    [0;32mif[0m [0mis_available[0m[0;34m([0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 370 [0;31m        [0;32mreturn[0m [0mtorch[0m[0;34m.[0m[0m_C[0m[0;34m.[0m[0m_cuda_getDeviceCount[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    371 [0;31m    [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> --Return--
2
> [0;32m/home/aabdujyo/softwares/anaconda3/envs/agt_env/lib/python3.8/site-packages/torch/cuda/__init__.py[0m(370)[0;36mdevice_count[0;34m()[0m
[0;32m    369 [0;31m    [0;32mif[0m [0mis_available[0m[0;34m([0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 370 [0;31m        [0;32mreturn[0m [0mtorch[0m[0;34m.[0m[0m_C[0m[0;34m.[0m[0m_cuda_getDeviceCount[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    371 [0;31m    [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/scratch/aabdujyo/activitygraph_transformer/src/main.py[0m(97)[0;36mmain[0;34m()[0m
[0;32m     96 [0;31m        [0;32mif[0m [0mtorch[0m[0;34m.[0m[0mcuda[0m[0;34m.[0m[0mdevice_count[0m[0;34m([0m[0;34m)[0m [0;34m>=[0m [0;36m1[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 97 [0;31m            [0mutils[0m[0;34m.[0m[0minit_distributed_mode[0m[0;34m([0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     98 [0;31m        [0mdevice[0m [0;34m=[0m [0mtorch[0m[0;34m.[0m[0mdevice[0m[0;34m([0m[0margs[0m[0;34m.[0m[0mdevice[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> --Call--
> [0;32m/scratch/aabdujyo/activitygraph_transformer/src/utils/misc.py[0m(363)[0;36minit_distributed_mode[0;34m()[0m
[0;32m    362 [0;31m[0;34m[0m[0m
[0m[0;32m--> 363 [0;31m[0;32mdef[0m [0minit_distributed_mode[0m[0;34m([0m[0margs[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    364 [0;31m    [0;32mif[0m [0;34m'RANK'[0m [0;32min[0m [0mos[0m[0;34m.[0m[0menviron[0m [0;32mand[0m [0;34m'WORLD_SIZE'[0m [0;32min[0m [0mos[0m[0;34m.[0m[0menviron[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/scratch/aabdujyo/activitygraph_transformer/src/utils/misc.py[0m(364)[0;36minit_distributed_mode[0;34m()[0m
[0;32m    363 [0;31m[0;32mdef[0m [0minit_distributed_mode[0m[0;34m([0m[0margs[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 364 [0;31m    [0;32mif[0m [0;34m'RANK'[0m [0;32min[0m [0mos[0m[0;34m.[0m[0menviron[0m [0;32mand[0m [0;34m'WORLD_SIZE'[0m [0;32min[0m [0mos[0m[0;34m.[0m[0menviron[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    365 [0;31m        [0margs[0m[0;34m.[0m[0mrank[0m [0;34m=[0m [0mint[0m[0;34m([0m[0mos[0m[0;34m.[0m[0menviron[0m[0;34m[[0m[0;34m"RANK"[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/scratch/aabdujyo/activitygraph_transformer/src/utils/misc.py[0m(368)[0;36minit_distributed_mode[0;34m()[0m
[0;32m    367 [0;31m        [0margs[0m[0;34m.[0m[0mgpu[0m [0;34m=[0m [0mint[0m[0;34m([0m[0mos[0m[0;34m.[0m[0menviron[0m[0;34m[[0m[0;34m'LOCAL_RANK'[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 368 [0;31m    [0;32melif[0m [0;34m'SLURM_PROCID'[0m [0;32min[0m [0mos[0m[0;34m.[0m[0menviron[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    369 [0;31m        [0margs[0m[0;34m.[0m[0mrank[0m [0;34m=[0m [0mint[0m[0;34m([0m[0mos[0m[0;34m.[0m[0menviron[0m[0;34m[[0m[0;34m'SLURM_PROCID'[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/scratch/aabdujyo/activitygraph_transformer/src/utils/misc.py[0m(369)[0;36minit_distributed_mode[0;34m()[0m
[0;32m    368 [0;31m    [0;32melif[0m [0;34m'SLURM_PROCID'[0m [0;32min[0m [0mos[0m[0;34m.[0m[0menviron[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 369 [0;31m        [0margs[0m[0;34m.[0m[0mrank[0m [0;34m=[0m [0mint[0m[0;34m([0m[0mos[0m[0;34m.[0m[0menviron[0m[0;34m[[0m[0;34m'SLURM_PROCID'[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    370 [0;31m        [0margs[0m[0;34m.[0m[0mgpu[0m [0;34m=[0m [0margs[0m[0;34m.[0m[0mrank[0m [0;34m%[0m [0mtorch[0m[0;34m.[0m[0mcuda[0m[0;34m.[0m[0mdevice_count[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/scratch/aabdujyo/activitygraph_transformer/src/utils/misc.py[0m(370)[0;36minit_distributed_mode[0;34m()[0m
[0;32m    369 [0;31m        [0margs[0m[0;34m.[0m[0mrank[0m [0;34m=[0m [0mint[0m[0;34m([0m[0mos[0m[0;34m.[0m[0menviron[0m[0;34m[[0m[0;34m'SLURM_PROCID'[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 370 [0;31m        [0margs[0m[0;34m.[0m[0mgpu[0m [0;34m=[0m [0margs[0m[0;34m.[0m[0mrank[0m [0;34m%[0m [0mtorch[0m[0;34m.[0m[0mcuda[0m[0;34m.[0m[0mdevice_count[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    371 [0;31m    [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/scratch/aabdujyo/activitygraph_transformer/src/utils/misc.py[0m(376)[0;36minit_distributed_mode[0;34m()[0m
[0;32m    375 [0;31m[0;34m[0m[0m
[0m[0;32m--> 376 [0;31m    [0margs[0m[0;34m.[0m[0mdistributed[0m [0;34m=[0m [0;32mTrue[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    377 [0;31m    [0mtorch[0m[0;34m.[0m[0mcuda[0m[0;34m.[0m[0mset_device[0m[0;34m([0m[0margs[0m[0;34m.[0m[0mgpu[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> 0
ipdb> args = Namespace(activation='leaky_relu', aux_loss=False, batch_size=4, clip_max_norm=0.0, cuda=True, data_root='/home/aabdujyo/scratch/activitygraph_transformer/data/thumos', dataset='thumos', dec_layers=4, device='cpu', dim_feedforward=1024, dim_latent=128, dist_url='env://', dropout=0.0, enc_layers=4, eos_coef=0.1, epochs=10, eval=False, features='i3d_feats', gpu=0, hidden_dim=256, local_rank=None, lr=1e-05, lr_drop=1500, lr_joiner=1e-05, model='agt', nheads=4, norm_type='bn', num_classes=20, num_inputs=512, num_pos_embed_dict=512, num_queries=500, num_workers=0, output_dir='output/checkpoints_/checkpoints_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512', position_embedding='learned', pre_norm=False, rank=0, resume='', sample_rate=4, save_checkpoint_every=30, seed=42, segment_loss_coef=5.0, set_cost_class=1, set_cost_segment=5.0, set_cost_siou=3.0, siou_loss_coef=3.0, start_epoch=0, step_size=1, weight_decay=0.0, world_size=1)
ipdb> 0
ipdb> > [0;32m/scratch/aabdujyo/activitygraph_transformer/src/utils/misc.py[0m(377)[0;36minit_distributed_mode[0;34m()[0m
[0;32m    376 [0;31m    [0margs[0m[0;34m.[0m[0mdistributed[0m [0;34m=[0m [0;32mTrue[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 377 [0;31m    [0mtorch[0m[0;34m.[0m[0mcuda[0m[0;34m.[0m[0mset_device[0m[0;34m([0m[0margs[0m[0;34m.[0m[0mgpu[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    378 [0;31m    [0margs[0m[0;34m.[0m[0mdist_backend[0m [0;34m=[0m [0;34m'nccl'[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/scratch/aabdujyo/activitygraph_transformer/src/utils/misc.py[0m(378)[0;36minit_distributed_mode[0;34m()[0m
[0;32m    377 [0;31m    [0mtorch[0m[0;34m.[0m[0mcuda[0m[0;34m.[0m[0mset_device[0m[0;34m([0m[0margs[0m[0;34m.[0m[0mgpu[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 378 [0;31m    [0margs[0m[0;34m.[0m[0mdist_backend[0m [0;34m=[0m [0;34m'nccl'[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    379 [0;31m    [0mprint[0m[0;34m([0m[0;34m'| distributed init (rank {}): {}'[0m[0;34m.[0m[0mformat[0m[0;34m([0m[0margs[0m[0;34m.[0m[0mrank[0m[0;34m,[0m [0margs[0m[0;34m.[0m[0mdist_url[0m[0;34m)[0m[0;34m,[0m [0mflush[0m[0;34m=[0m[0;32mTrue[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/scratch/aabdujyo/activitygraph_transformer/src/utils/misc.py[0m(379)[0;36minit_distributed_mode[0;34m()[0m
[0;32m    378 [0;31m    [0margs[0m[0;34m.[0m[0mdist_backend[0m [0;34m=[0m [0;34m'nccl'[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 379 [0;31m    [0mprint[0m[0;34m([0m[0;34m'| distributed init (rank {}): {}'[0m[0;34m.[0m[0mformat[0m[0;34m([0m[0margs[0m[0;34m.[0m[0mrank[0m[0;34m,[0m [0margs[0m[0;34m.[0m[0mdist_url[0m[0;34m)[0m[0;34m,[0m [0mflush[0m[0;34m=[0m[0;32mTrue[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    380 [0;31m    [0mtorch[0m[0;34m.[0m[0mdistributed[0m[0;34m.[0m[0minit_process_group[0m[0;34m([0m[0mbackend[0m[0;34m=[0m[0margs[0m[0;34m.[0m[0mdist_backend[0m[0;34m,[0m [0minit_method[0m[0;34m=[0m[0margs[0m[0;34m.[0m[0mdist_url[0m[0;34m,[0m[0mworld_size[0m[0;34m=[0m[0margs[0m[0;34m.[0m[0mworld_size[0m[0;34m,[0m [0mrank[0m[0;34m=[0m[0margs[0m[0;34m.[0m[0mrank[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> | distributed init (rank 0): env://
> [0;32m/scratch/aabdujyo/activitygraph_transformer/src/utils/misc.py[0m(380)[0;36minit_distributed_mode[0;34m()[0m
[0;32m    379 [0;31m    [0mprint[0m[0;34m([0m[0;34m'| distributed init (rank {}): {}'[0m[0;34m.[0m[0mformat[0m[0;34m([0m[0margs[0m[0;34m.[0m[0mrank[0m[0;34m,[0m [0margs[0m[0;34m.[0m[0mdist_url[0m[0;34m)[0m[0;34m,[0m [0mflush[0m[0;34m=[0m[0;32mTrue[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 380 [0;31m    [0mtorch[0m[0;34m.[0m[0mdistributed[0m[0;34m.[0m[0minit_process_group[0m[0;34m([0m[0mbackend[0m[0;34m=[0m[0margs[0m[0;34m.[0m[0mdist_backend[0m[0;34m,[0m [0minit_method[0m[0;34m=[0m[0margs[0m[0;34m.[0m[0mdist_url[0m[0;34m,[0m[0mworld_size[0m[0;34m=[0m[0margs[0m[0;34m.[0m[0mworld_size[0m[0;34m,[0m [0mrank[0m[0;34m=[0m[0margs[0m[0;34m.[0m[0mrank[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    381 [0;31m    [0;32mif[0m [0;32mnot[0m [0margs[0m[0;34m.[0m[0mmp[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> ValueError: Error initializing torch.distributed using env:// rendezvous: environment variable MASTER_ADDR expected, but not set
> [0;32m/scratch/aabdujyo/activitygraph_transformer/src/utils/misc.py[0m(380)[0;36minit_distributed_mode[0;34m()[0m
[0;32m    379 [0;31m    [0mprint[0m[0;34m([0m[0;34m'| distributed init (rank {}): {}'[0m[0;34m.[0m[0mformat[0m[0;34m([0m[0margs[0m[0;34m.[0m[0mrank[0m[0;34m,[0m [0margs[0m[0;34m.[0m[0mdist_url[0m[0;34m)[0m[0;34m,[0m [0mflush[0m[0;34m=[0m[0;32mTrue[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 380 [0;31m    [0mtorch[0m[0;34m.[0m[0mdistributed[0m[0;34m.[0m[0minit_process_group[0m[0;34m([0m[0mbackend[0m[0;34m=[0m[0margs[0m[0;34m.[0m[0mdist_backend[0m[0;34m,[0m [0minit_method[0m[0;34m=[0m[0margs[0m[0;34m.[0m[0mdist_url[0m[0;34m,[0m[0mworld_size[0m[0;34m=[0m[0margs[0m[0;34m.[0m[0mworld_size[0m[0;34m,[0m [0mrank[0m[0;34m=[0m[0margs[0m[0;34m.[0m[0mrank[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    381 [0;31m    [0;32mif[0m [0;32mnot[0m [0margs[0m[0;34m.[0m[0mmp[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> Exiting Debugger.
Logging output to output/logs/log_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512.log
Namespace(activation='leaky_relu', aux_loss=False, batch_size=4, clip_max_norm=0.0, cuda=True, data_root='/home/aabdujyo/scratch/activitygraph_transformer/data/thumos', dataset='thumos', dec_layers=4, device='cpu', dim_feedforward=1024, dim_latent=128, dist_url='env://', dropout=0.0, enc_layers=4, eos_coef=0.1, epochs=10, eval=False, features='i3d_feats', hidden_dim=256, local_rank=None, lr=1e-05, lr_drop=1500, lr_joiner=1e-05, model='agt', nheads=4, norm_type='bn', num_classes=20, num_inputs=512, num_pos_embed_dict=512, num_queries=500, num_workers=0, output_dir='output/checkpoints_/checkpoints_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512', position_embedding='learned', pre_norm=False, resume='', sample_rate=4, save_checkpoint_every=30, seed=42, segment_loss_coef=5.0, set_cost_class=1, set_cost_segment=5.0, set_cost_siou=3.0, siou_loss_coef=3.0, start_epoch=0, step_size=1, weight_decay=0.0, world_size=1)
| distributed init (rank 0): env://
Traceback (most recent call last):
  File "src/main.py", line 206, in <module>
    main(args)
  File "src/main.py", line 97, in main
    utils.init_distributed_mode(args)
  File "/scratch/aabdujyo/activitygraph_transformer/src/utils/misc.py", line 380, in init_distributed_mode
    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,world_size=args.world_size, rank=args.rank)
  File "/home/aabdujyo/softwares/anaconda3/envs/agt_env/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 397, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/home/aabdujyo/softwares/anaconda3/envs/agt_env/lib/python3.8/site-packages/torch/distributed/rendezvous.py", line 155, in _env_rendezvous_handler
    raise _env_error("MASTER_ADDR")
ValueError: Error initializing torch.distributed using env:// rendezvous: environment variable MASTER_ADDR expected, but not set
Logging output to output/logs/log_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512.log
Namespace(activation='leaky_relu', aux_loss=False, batch_size=4, clip_max_norm=0.0, cuda=True, data_root='/home/aabdujyo/scratch/activitygraph_transformer/data/thumos', dataset='thumos', dec_layers=4, device='cpu', dim_feedforward=1024, dim_latent=128, dist_url='env://', dropout=0.0, enc_layers=4, eos_coef=0.1, epochs=10, eval=False, features='i3d_feats', hidden_dim=256, local_rank=None, lr=1e-05, lr_drop=1500, lr_joiner=1e-05, model='agt', nheads=4, norm_type='bn', num_classes=20, num_inputs=512, num_pos_embed_dict=512, num_queries=500, num_workers=0, output_dir='output/checkpoints_/checkpoints_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512', position_embedding='learned', pre_norm=False, resume='', sample_rate=4, save_checkpoint_every=30, seed=42, segment_loss_coef=5.0, set_cost_class=1, set_cost_segment=5.0, set_cost_siou=3.0, siou_loss_coef=3.0, start_epoch=0, step_size=1, weight_decay=0.0, world_size=1)
| distributed init (rank 0): env://
Traceback (most recent call last):
  File "src/main.py", line 206, in <module>
    main(args)
  File "src/main.py", line 97, in main
    utils.init_distributed_mode(args)
  File "/scratch/aabdujyo/activitygraph_transformer/src/utils/misc.py", line 380, in init_distributed_mode
    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,world_size=args.world_size, rank=args.rank)
  File "/home/aabdujyo/softwares/anaconda3/envs/agt_env/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 397, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/home/aabdujyo/softwares/anaconda3/envs/agt_env/lib/python3.8/site-packages/torch/distributed/rendezvous.py", line 155, in _env_rendezvous_handler
    raise _env_error("MASTER_ADDR")
ValueError: Error initializing torch.distributed using env:// rendezvous: environment variable MASTER_ADDR expected, but not set
Logging output to output/logs/log_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512.log
Logging output to output/logs/log_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512.log
Namespace(activation='leaky_relu', aux_loss=False, batch_size=4, clip_max_norm=0.0, cuda=True, data_root='/home/aabdujyo/scratch/activitygraph_transformer/data/thumos', dataset='thumos', dec_layers=4, device='cpu', dim_feedforward=1024, dim_latent=128, dist_url='env://', dropout=0.0, enc_layers=4, eos_coef=0.1, epochs=10, eval=False, features='i3d_feats', hidden_dim=256, local_rank=None, lr=1e-05, lr_drop=1500, lr_joiner=1e-05, model='agt', nheads=4, norm_type='bn', num_classes=20, num_inputs=512, num_pos_embed_dict=512, num_queries=500, num_workers=0, output_dir='output/checkpoints_/checkpoints_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512', position_embedding='learned', pre_norm=False, resume='', sample_rate=4, save_checkpoint_every=30, seed=42, segment_loss_coef=5.0, set_cost_class=1, set_cost_segment=5.0, set_cost_siou=3.0, siou_loss_coef=3.0, start_epoch=0, step_size=1, weight_decay=0.0, world_size=1)
> [0;32m/scratch/aabdujyo/activitygraph_transformer/src/main.py[0m(96)[0;36mmain[0;34m()[0m
[0;32m     95 [0;31m        [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 96 [0;31m        [0;32mif[0m [0mtorch[0m[0;34m.[0m[0mcuda[0m[0;34m.[0m[0mdevice_count[0m[0;34m([0m[0;34m)[0m [0;34m>=[0m [0;36m3[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     97 [0;31m            [0mutils[0m[0;34m.[0m[0minit_distributed_mode[0m[0;34m([0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> 2
ipdb> > [0;32m/scratch/aabdujyo/activitygraph_transformer/src/main.py[0m(98)[0;36mmain[0;34m()[0m
[0;32m     97 [0;31m            [0mutils[0m[0;34m.[0m[0minit_distributed_mode[0m[0;34m([0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 98 [0;31m        [0mdevice[0m [0;34m=[0m [0mtorch[0m[0;34m.[0m[0mdevice[0m[0;34m([0m[0margs[0m[0;34m.[0m[0mdevice[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     99 [0;31m    [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/scratch/aabdujyo/activitygraph_transformer/src/main.py[0m(103)[0;36mmain[0;34m()[0m
[0;32m    102 [0;31m    [0;31m# fix the seed for reproducibility[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 103 [0;31m    [0;32mif[0m [0margs[0m[0;34m.[0m[0mcuda[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    104 [0;31m        [0mseed[0m [0;34m=[0m [0margs[0m[0;34m.[0m[0mseed[0m [0;34m+[0m [0mutils[0m[0;34m.[0m[0mget_rank[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/scratch/aabdujyo/activitygraph_transformer/src/main.py[0m(104)[0;36mmain[0;34m()[0m
[0;32m    103 [0;31m    [0;32mif[0m [0margs[0m[0;34m.[0m[0mcuda[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 104 [0;31m        [0mseed[0m [0;34m=[0m [0margs[0m[0;34m.[0m[0mseed[0m [0;34m+[0m [0mutils[0m[0;34m.[0m[0mget_rank[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    105 [0;31m    [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/scratch/aabdujyo/activitygraph_transformer/src/main.py[0m(107)[0;36mmain[0;34m()[0m
[0;32m    106 [0;31m        [0mseed[0m [0;34m=[0m [0margs[0m[0;34m.[0m[0mseed[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 107 [0;31m    [0mtorch[0m[0;34m.[0m[0mmanual_seed[0m[0;34m([0m[0mseed[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    108 [0;31m    [0mnp[0m[0;34m.[0m[0mrandom[0m[0;34m.[0m[0mseed[0m[0;34m([0m[0mseed[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/scratch/aabdujyo/activitygraph_transformer/src/main.py[0m(108)[0;36mmain[0;34m()[0m
[0;32m    107 [0;31m    [0mtorch[0m[0;34m.[0m[0mmanual_seed[0m[0;34m([0m[0mseed[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 108 [0;31m    [0mnp[0m[0;34m.[0m[0mrandom[0m[0;34m.[0m[0mseed[0m[0;34m([0m[0mseed[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    109 [0;31m    [0mrandom[0m[0;34m.[0m[0mseed[0m[0;34m([0m[0mseed[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> > [0;32m/scratch/aabdujyo/activitygraph_transformer/src/main.py[0m(109)[0;36mmain[0;34m()[0m
[0;32m    108 [0;31m    [0mnp[0m[0;34m.[0m[0mrandom[0m[0;34m.[0m[0mseed[0m[0;34m([0m[0mseed[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 109 [0;31m    [0mrandom[0m[0;34m.[0m[0mseed[0m[0;34m([0m[0mseed[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    110 [0;31m[0;34m[0m[0m
[0m
ipdb> Exiting Debugger.
Logging output to output/logs/log_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512.log
Namespace(activation='leaky_relu', aux_loss=False, batch_size=4, clip_max_norm=0.0, cuda=True, data_root='/home/aabdujyo/scratch/activitygraph_transformer/data/thumos', dataset='thumos', dec_layers=4, device='cpu', dim_feedforward=1024, dim_latent=128, dist_url='env://', dropout=0.0, enc_layers=4, eos_coef=0.1, epochs=10, eval=False, features='i3d_feats', hidden_dim=256, local_rank=None, lr=1e-05, lr_drop=1500, lr_joiner=1e-05, model='agt', nheads=4, norm_type='bn', num_classes=20, num_inputs=512, num_pos_embed_dict=512, num_queries=500, num_workers=0, output_dir='output/checkpoints_/checkpoints_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512', position_embedding='learned', pre_norm=False, resume='', sample_rate=4, save_checkpoint_every=30, seed=42, segment_loss_coef=5.0, set_cost_class=1, set_cost_segment=5.0, set_cost_siou=3.0, siou_loss_coef=3.0, start_epoch=0, step_size=1, weight_decay=0.0, world_size=1)
> [0;32m/scratch/aabdujyo/activitygraph_transformer/src/main.py[0m(96)[0;36mmain[0;34m()[0m
[0;32m     95 [0;31m        [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 96 [0;31m        [0;32mif[0m [0mtorch[0m[0;34m.[0m[0mcuda[0m[0;34m.[0m[0mdevice_count[0m[0;34m([0m[0;34m)[0m [0;34m>=[0m [0;36m3[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     97 [0;31m            [0mutils[0m[0;34m.[0m[0minit_distributed_mode[0m[0;34m([0m[0margs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> Exiting Debugger.
Logging output to output/logs/log_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512.log
Namespace(activation='leaky_relu', aux_loss=False, batch_size=4, clip_max_norm=0.0, cuda=True, data_root='/home/aabdujyo/scratch/activitygraph_transformer/data/thumos', dataset='thumos', dec_layers=4, device='cpu', dim_feedforward=1024, dim_latent=128, dist_url='env://', dropout=0.0, enc_layers=4, eos_coef=0.1, epochs=10, eval=False, features='i3d_feats', hidden_dim=256, local_rank=None, lr=1e-05, lr_drop=1500, lr_joiner=1e-05, model='agt', nheads=4, norm_type='bn', num_classes=20, num_inputs=512, num_pos_embed_dict=512, num_queries=500, num_workers=0, output_dir='output/checkpoints_/checkpoints_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512', position_embedding='learned', pre_norm=False, resume='', sample_rate=4, save_checkpoint_every=30, seed=42, segment_loss_coef=5.0, set_cost_class=1, set_cost_segment=5.0, set_cost_siou=3.0, siou_loss_coef=3.0, start_epoch=0, step_size=1, weight_decay=0.0, world_size=1)
AGT(
  (transformer): Graph_Transformer(
    (encoder): GraphTransformerEncoder(
      (layers): ModuleList(
        (0): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (1): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (2): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (3): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (decoder): GraphTransformerDecoder(
      (layers): ModuleList(
        (0): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
        (1): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
        (2): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
        (3): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
      )
      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
  )
  (joiner): Joiner(
    (0): PositionEmbeddingLearned(
      (row_embed): Embedding(512, 256)
    )
  )
  (query_embed): Embedding(500, 256)
  (class_embed): Linear(in_features=256, out_features=21, bias=True)
  (segments_embed): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
      (2): Linear(in_features=256, out_features=2, bias=True)
    )
  )
  (input_proj): Conv1d(2048, 256, kernel_size=(1,), stride=(1,))
)
Traceback (most recent call last):
  File "src/main.py", line 206, in <module>
    main(args)
  File "src/main.py", line 115, in main
    if args.cuda and args.distributed:
AttributeError: 'Namespace' object has no attribute 'distributed'
Logging output to output/logs/log_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512.log
Namespace(activation='leaky_relu', aux_loss=False, batch_size=4, clip_max_norm=0.0, cuda=True, data_root='/home/aabdujyo/scratch/activitygraph_transformer/data/thumos', dataset='thumos', dec_layers=4, device='cpu', dim_feedforward=1024, dim_latent=128, dist_url='env://', distributed=False, dropout=0.0, enc_layers=4, eos_coef=0.1, epochs=10, eval=False, features='i3d_feats', hidden_dim=256, local_rank=None, lr=1e-05, lr_drop=1500, lr_joiner=1e-05, model='agt', nheads=4, norm_type='bn', num_classes=20, num_inputs=512, num_pos_embed_dict=512, num_queries=500, num_workers=0, output_dir='output/checkpoints_/checkpoints_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512', position_embedding='learned', pre_norm=False, resume='', sample_rate=4, save_checkpoint_every=30, seed=42, segment_loss_coef=5.0, set_cost_class=1, set_cost_segment=5.0, set_cost_siou=3.0, siou_loss_coef=3.0, start_epoch=0, step_size=1, weight_decay=0.0, world_size=1)
AGT(
  (transformer): Graph_Transformer(
    (encoder): GraphTransformerEncoder(
      (layers): ModuleList(
        (0): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (1): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (2): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (3): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (decoder): GraphTransformerDecoder(
      (layers): ModuleList(
        (0): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
        (1): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
        (2): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
        (3): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
      )
      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
  )
  (joiner): Joiner(
    (0): PositionEmbeddingLearned(
      (row_embed): Embedding(512, 256)
    )
  )
  (query_embed): Embedding(500, 256)
  (class_embed): Linear(in_features=256, out_features=21, bias=True)
  (segments_embed): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
      (2): Linear(in_features=256, out_features=2, bias=True)
    )
  )
  (input_proj): Conv1d(2048, 256, kernel_size=(1,), stride=(1,))
)Logging output to output/logs/log_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512.log
Traceback (most recent call last):
  File "src/main.py", line 10, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'
Logging output to output/logs/log_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512.log
Namespace(activation='leaky_relu', aux_loss=False, batch_size=4, clip_max_norm=0.0, cuda=True, data_root='/home/aabdujyo/scratch/activitygraph_transformer/data/thumos', dataset='thumos', dec_layers=4, device='0', dim_feedforward=1024, dim_latent=128, dist_url='env://', distributed=False, dropout=0.0, enc_layers=4, eos_coef=0.1, epochs=10, eval=False, features='i3d_feats', hidden_dim=256, local_rank=None, lr=1e-05, lr_drop=1500, lr_joiner=1e-05, model='agt', nheads=4, norm_type='bn', num_classes=20, num_inputs=512, num_pos_embed_dict=512, num_queries=500, num_workers=0, output_dir='output/checkpoints_/checkpoints_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512', position_embedding='learned', pre_norm=False, resume='', sample_rate=4, save_checkpoint_every=30, seed=42, segment_loss_coef=5.0, set_cost_class=1, set_cost_segment=5.0, set_cost_siou=3.0, siou_loss_coef=3.0, start_epoch=0, step_size=1, weight_decay=0.0, world_size=1)
| distributed init (rank 0): env://
Traceback (most recent call last):
  File "src/main.py", line 209, in <module>
    main(args)
  File "src/main.py", line 99, in main
    utils.init_distributed_mode(args)
  File "/scratch/aabdujyo/activitygraph_transformer/src/utils/misc.py", line 380, in init_distributed_mode
    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,world_size=args.world_size, rank=args.rank)
  File "/home/aabdujyo/softwares/anaconda3/envs/agt_env/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 397, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/home/aabdujyo/softwares/anaconda3/envs/agt_env/lib/python3.8/site-packages/torch/distributed/rendezvous.py", line 155, in _env_rendezvous_handler
    raise _env_error("MASTER_ADDR")
ValueError: Error initializing torch.distributed using env:// rendezvous: environment variable MASTER_ADDR expected, but not set
Logging output to output/logs/log_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512.log
Namespace(activation='leaky_relu', aux_loss=False, batch_size=4, clip_max_norm=0.0, cuda=True, data_root='/home/aabdujyo/scratch/activitygraph_transformer/data/thumos', dataset='thumos', dec_layers=4, device='0', dim_feedforward=1024, dim_latent=128, dist_url='env://', distributed=False, dropout=0.0, enc_layers=4, eos_coef=0.1, epochs=10, eval=False, features='i3d_feats', hidden_dim=256, local_rank=None, lr=1e-05, lr_drop=1500, lr_joiner=1e-05, model='agt', nheads=4, norm_type='bn', num_classes=20, num_inputs=512, num_pos_embed_dict=512, num_queries=500, num_workers=0, output_dir='output/checkpoints_/checkpoints_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512', position_embedding='learned', pre_norm=False, resume='', sample_rate=4, save_checkpoint_every=30, seed=42, segment_loss_coef=5.0, set_cost_class=1, set_cost_segment=5.0, set_cost_siou=3.0, siou_loss_coef=3.0, start_epoch=0, step_size=1, weight_decay=0.0, world_size=1)
Traceback (most recent call last):
  File "src/main.py", line 209, in <module>
    main(args)
  File "src/main.py", line 100, in main
    device = torch.device(args.device) 
RuntimeError: Expected one of cpu, cuda, mkldnn, opengl, opencl, ideep, hip, msnpu device type at start of device string: 0
Logging output to output/logs/log_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512.log
Namespace(activation='leaky_relu', aux_loss=False, batch_size=4, clip_max_norm=0.0, cuda=True, data_root='/home/aabdujyo/scratch/activitygraph_transformer/data/thumos', dataset='thumos', dec_layers=4, device='cuda', dim_feedforward=1024, dim_latent=128, dist_url='env://', distributed=False, dropout=0.0, enc_layers=4, eos_coef=0.1, epochs=10, eval=False, features='i3d_feats', hidden_dim=256, local_rank=None, lr=1e-05, lr_drop=1500, lr_joiner=1e-05, model='agt', nheads=4, norm_type='bn', num_classes=20, num_inputs=512, num_pos_embed_dict=512, num_queries=500, num_workers=0, output_dir='output/checkpoints_/checkpoints_numqueries500_lr1e-5_lrdrop1500_dropout0_clipmaxnorm0_weightdecay0_posemblearned_lrjoiner1e-5_nheads4_nenclayers4_ndeclayers4_hdim256_sr4_batchsize4_nposembdict512_numinputs512', position_embedding='learned', pre_norm=False, resume='', sample_rate=4, save_checkpoint_every=30, seed=42, segment_loss_coef=5.0, set_cost_class=1, set_cost_segment=5.0, set_cost_siou=3.0, siou_loss_coef=3.0, start_epoch=0, step_size=1, weight_decay=0.0, world_size=1)
AGT(
  (transformer): Graph_Transformer(
    (encoder): GraphTransformerEncoder(
      (layers): ModuleList(
        (0): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (1): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (2): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
        (3): GraphTransformerEncoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (decoder): GraphTransformerDecoder(
      (layers): ModuleList(
        (0): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
        (1): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
        (2): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
        (3): GraphTransformerDecoderLayer(
          (self_attn): GraphSelfAttention(
            (attention_0): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (graph_self_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (multihead_attn): GraphEncoderDecoderAttention(
            (attention_0): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_1): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_2): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (attention_3): GraphEncoderDecoderAttentionLayer(
              (leakyrelu): LeakyReLU(negative_slope=0.1)
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (norm1): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (norm2): Norm(
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (graph_multihead_attn): MultiheadAttention(
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
          (dropout3): Dropout(p=0.0, inplace=False)
        )
      )
      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
  )
  (joiner): Joiner(
    (0): PositionEmbeddingLearned(
      (row_embed): Embedding(512, 256)
    )
  )
  (query_embed): Embedding(500, 256)
  (class_embed): Linear(in_features=256, out_features=21, bias=True)
  (segments_embed): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
      (2): Linear(in_features=256, out_features=2, bias=True)
    )
  )
  (input_proj): Conv1d(2048, 256, kernel_size=(1,), stride=(1,))
)